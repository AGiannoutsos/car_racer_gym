{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Copy of reinforcement_q_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vZq4VjDqVrms"
      ]
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d74dbc21f6a74b118728588f03cfbb93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_05ec633391c544dc914bb14e0f4b5879",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bd9972aaeb154e0d889a035fafad48f8",
              "IPY_MODEL_667ed7b7be5c44d293ab87e798086993"
            ]
          }
        },
        "05ec633391c544dc914bb14e0f4b5879": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd9972aaeb154e0d889a035fafad48f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_233b81de5b4c46d690ef011db8042583",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e7c3b4c79294ee4ae87407ea0cfe09c"
          }
        },
        "667ed7b7be5c44d293ab87e798086993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_53a0ed84b7dd41e7ae6bb83ae2347bde",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_583a4db6d9074477b2930a37d49707e6"
          }
        },
        "233b81de5b4c46d690ef011db8042583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e7c3b4c79294ee4ae87407ea0cfe09c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "53a0ed84b7dd41e7ae6bb83ae2347bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "583a4db6d9074477b2930a37d49707e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "30O9g6JfcRWz"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0yIFeqjcRXB"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBqYjRFyngvS"
      },
      "source": [
        "## Î¹Î½ÏƒÏ„Î±Î»Î»Ïƒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXLEWcW9_mGv"
      },
      "source": [
        "%%capture\n",
        "!sudo apt update && sudo apt install python-opengl\n",
        "!sudo apt update && sudo apt install xvfb\n",
        "!pip install gym-notebook-wrapper stable-baselines[mpi] box2d box2d-kengz pyvirtualdisplay pyglet\n",
        "!pip install gym\n",
        "!pip install wandb\n",
        "!pip install gym pyvirtualdisplay -qq\n",
        "!pip install folium==0.2.1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg -qq\n",
        "project=\"gym_car_racer\"\n",
        "entity=\"andreas_giannoutsos\"\n",
        "import wandb\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5BmHLRuAXyL"
      },
      "source": [
        "!git clone https://github.com/openai/gym.git\n",
        "%cd gym\n",
        "!pip install -e .\n",
        "!pip install stable-baselines[mpi]\n",
        "!pip install stable-baselines3[extra]\n",
        "# !pip install numpy==1.15.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtC2hX1EcRXE"
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import gnwrapper\n",
        "\n",
        "\n",
        "env = gnwrapper.Animation(gym.make('CartPole-v1'))\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLwEcu9DcRXG"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSeVgL-icRXI"
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTVk3C1VC4q1"
      },
      "source": [
        "# memory"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UtYrPmQcRXK"
      },
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUOYgW9ycRXL"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "        # Number of Linear input connections depends on output of conv2d layers\n",
        "        # and therefore the input image size, so compute it.\n",
        "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySdqfHsxcRXN"
      },
      "source": [
        "Input extraction\n",
        "^^^^^^^^^^^^^^^^\n",
        "\n",
        "The code below are utilities for extracting and processing rendered\n",
        "images from the environment. It uses the ``torchvision`` package, which\n",
        "makes it easy to compose image transforms. Once you run the cell it will\n",
        "display an example patch that it extracted.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZq4VjDqVrms"
      },
      "source": [
        "## CaR REWREC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRplH8vdVpw0"
      },
      "source": [
        "import sys, math\n",
        "import numpy as np\n",
        "\n",
        "import Box2D\n",
        "from Box2D.b2 import (edgeShape, circleShape, fixtureDef, polygonShape, revoluteJointDef, contactListener)\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.envs.box2d.car_dynamics import Car\n",
        "from gym.utils import colorize, seeding, EzPickle\n",
        "\n",
        "import pyglet\n",
        "from pyglet import gl\n",
        "\n",
        "# Easiest continuous control task to learn from pixels, a top-down racing environment.\n",
        "# Discrete control is reasonable in this environment as well, on/off discretization is\n",
        "# fine.\n",
        "#\n",
        "# State consists of STATE_W x STATE_H pixels.\n",
        "#\n",
        "# Reward is -0.1 every frame and +1000/N for every track tile visited, where N is\n",
        "# the total number of tiles visited in the track. For example, if you have finished in 732 frames,\n",
        "# your reward is 1000 - 0.1*732 = 926.8 points.\n",
        "#\n",
        "# Game is solved when agent consistently gets 900+ points. Track generated is random every episode.\n",
        "#\n",
        "# Episode finishes when all tiles are visited. Car also can go outside of PLAYFIELD, that\n",
        "# is far off the track, then it will get -100 and die.\n",
        "#\n",
        "# Some indicators shown at the bottom of the window and the state RGB buffer. From\n",
        "# left to right: true speed, four ABS sensors, steering wheel position and gyroscope.\n",
        "#\n",
        "# To play yourself (it's rather fast for humans), type:\n",
        "#\n",
        "# python gym/envs/box2d/car_racing.py\n",
        "#\n",
        "# Remember it's powerful rear-wheel drive car, don't press accelerator and turn at the\n",
        "# same time.\n",
        "#\n",
        "# Created by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.\n",
        "\n",
        "STATE_W = 96   # less than Atari 160x192\n",
        "STATE_H = 96\n",
        "VIDEO_W = 600\n",
        "VIDEO_H = 400\n",
        "WINDOW_W = 1000\n",
        "WINDOW_H = 800\n",
        "\n",
        "SCALE       = 6.0        # Track scale\n",
        "TRACK_RAD   = 900/SCALE  # Track is heavily morphed circle with this radius\n",
        "PLAYFIELD   = 2000/SCALE # Game over boundary\n",
        "FPS         = 50         # Frames per second\n",
        "ZOOM        = 2.7        # Camera zoom\n",
        "ZOOM_FOLLOW = True       # Set to False for fixed view (don't use zoom)\n",
        "\n",
        "\n",
        "TRACK_DETAIL_STEP = 21/SCALE\n",
        "TRACK_TURN_RATE = 0.31\n",
        "TRACK_WIDTH = 40/SCALE\n",
        "BORDER = 8/SCALE\n",
        "BORDER_MIN_COUNT = 4\n",
        "\n",
        "ROAD_COLOR = [0.4, 0.4, 0.4]\n",
        "\n",
        "class FrictionDetector(contactListener):\n",
        "    def __init__(self, env):\n",
        "        contactListener.__init__(self)\n",
        "        self.env = env\n",
        "    def BeginContact(self, contact):\n",
        "        self._contact(contact, True)\n",
        "    def EndContact(self, contact):\n",
        "        self._contact(contact, False)\n",
        "    def _contact(self, contact, begin):\n",
        "        tile = None\n",
        "        obj = None\n",
        "        u1 = contact.fixtureA.body.userData\n",
        "        u2 = contact.fixtureB.body.userData\n",
        "        if u1 and \"road_friction\" in u1.__dict__:\n",
        "            tile = u1\n",
        "            obj  = u2\n",
        "        if u2 and \"road_friction\" in u2.__dict__:\n",
        "            tile = u2\n",
        "            obj  = u1\n",
        "        if not tile:\n",
        "            return\n",
        "\n",
        "        tile.color[0] = ROAD_COLOR[0]\n",
        "        tile.color[1] = ROAD_COLOR[1]\n",
        "        tile.color[2] = ROAD_COLOR[2]\n",
        "        if not obj or \"tiles\" not in obj.__dict__:\n",
        "            return\n",
        "        if begin:\n",
        "            obj.tiles.add(tile)\n",
        "            # print tile.road_friction, \"ADD\", len(obj.tiles)\n",
        "            if not tile.road_visited:\n",
        "                tile.road_visited = True\n",
        "                self.env.reward += 1000.0/len(self.env.track)\n",
        "                self.env.tile_visited_count += 1\n",
        "        else:\n",
        "            obj.tiles.remove(tile)\n",
        "            # print tile.road_friction, \"DEL\", len(obj.tiles) -- should delete to zero when on grass (this works)\n",
        "\n",
        "class CarRacing(gym.Env, EzPickle):\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array', 'state_pixels'],\n",
        "        'video.frames_per_second' : FPS\n",
        "    }\n",
        "\n",
        "    def __init__(self, totoal_episode_steps=1000 ,verbose=1):\n",
        "        EzPickle.__init__(self)\n",
        "        self.seed()\n",
        "        self.contactListener_keepref = FrictionDetector(self)\n",
        "        self.world = Box2D.b2World((0,0), contactListener=self.contactListener_keepref)\n",
        "        self.viewer = None\n",
        "        self.invisible_state_window = None\n",
        "        self.invisible_video_window = None\n",
        "        self.road = None\n",
        "        self.car = None\n",
        "        self.reward = 0.0\n",
        "        self.prev_reward = 0.0\n",
        "        self.verbose = verbose\n",
        "        self.fd_tile = fixtureDef(\n",
        "                shape = polygonShape(vertices=\n",
        "                    [(0, 0),(1, 0),(1, -1),(0, -1)]))\n",
        "\n",
        "        # discrete car racer\n",
        "        # self.action_space = spaces.Box( np.array([-1,0,0]), np.array([+1,+1,+1]), dtype=np.float32)  # steer, gas, brake\n",
        "        self.action_space = spaces.Discrete(5)\n",
        "        self.actions = [np.array([-1,0,0], dtype=np.float32), \n",
        "                        np.array([1,0,0], dtype=np.float32), \n",
        "                        np.array([0,1,0], dtype=np.float32), \n",
        "                        np.array([0,0,0.8], dtype=np.float32), \n",
        "                        np.array([0,0,0], dtype=np.float32)]  # left right, gas, brake, nothing\n",
        "        self.totoal_episode_steps = totoal_episode_steps\n",
        "        self.current_steps = 0\n",
        "\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(96, 96, 3), dtype=np.uint8)\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def _destroy(self):\n",
        "        if not self.road:\n",
        "            return\n",
        "        for t in self.road:\n",
        "            self.world.DestroyBody(t)\n",
        "        self.road = []\n",
        "        self.car.destroy()\n",
        "\n",
        "    def _create_track(self):\n",
        "        CHECKPOINTS = 12\n",
        "\n",
        "        # Create checkpoints\n",
        "        checkpoints = []\n",
        "        for c in range(CHECKPOINTS):\n",
        "            alpha = 2*math.pi*c/CHECKPOINTS + self.np_random.uniform(0, 2*math.pi*1/CHECKPOINTS)\n",
        "            rad = self.np_random.uniform(TRACK_RAD/3, TRACK_RAD)\n",
        "            if c==0:\n",
        "                alpha = 0\n",
        "                rad = 1.5*TRACK_RAD\n",
        "            if c==CHECKPOINTS-1:\n",
        "                alpha = 2*math.pi*c/CHECKPOINTS\n",
        "                self.start_alpha = 2*math.pi*(-0.5)/CHECKPOINTS\n",
        "                rad = 1.5*TRACK_RAD\n",
        "            checkpoints.append( (alpha, rad*math.cos(alpha), rad*math.sin(alpha)) )\n",
        "\n",
        "        # print \"\\n\".join(str(h) for h in checkpoints)\n",
        "        # self.road_poly = [ (    # uncomment this to see checkpoints\n",
        "        #    [ (tx,ty) for a,tx,ty in checkpoints ],\n",
        "        #    (0.7,0.7,0.9) ) ]\n",
        "        self.road = []\n",
        "\n",
        "        # Go from one checkpoint to another to create track\n",
        "        x, y, beta = 1.5*TRACK_RAD, 0, 0\n",
        "        dest_i = 0\n",
        "        laps = 0\n",
        "        track = []\n",
        "        no_freeze = 2500\n",
        "        visited_other_side = False\n",
        "        while True:\n",
        "            alpha = math.atan2(y, x)\n",
        "            if visited_other_side and alpha > 0:\n",
        "                laps += 1\n",
        "                visited_other_side = False\n",
        "            if alpha < 0:\n",
        "                visited_other_side = True\n",
        "                alpha += 2*math.pi\n",
        "            while True: # Find destination from checkpoints\n",
        "                failed = True\n",
        "                while True:\n",
        "                    dest_alpha, dest_x, dest_y = checkpoints[dest_i % len(checkpoints)]\n",
        "                    if alpha <= dest_alpha:\n",
        "                        failed = False\n",
        "                        break\n",
        "                    dest_i += 1\n",
        "                    if dest_i % len(checkpoints) == 0:\n",
        "                        break\n",
        "                if not failed:\n",
        "                    break\n",
        "                alpha -= 2*math.pi\n",
        "                continue\n",
        "            r1x = math.cos(beta)\n",
        "            r1y = math.sin(beta)\n",
        "            p1x = -r1y\n",
        "            p1y = r1x\n",
        "            dest_dx = dest_x - x  # vector towards destination\n",
        "            dest_dy = dest_y - y\n",
        "            proj = r1x*dest_dx + r1y*dest_dy  # destination vector projected on rad\n",
        "            while beta - alpha >  1.5*math.pi:\n",
        "                 beta -= 2*math.pi\n",
        "            while beta - alpha < -1.5*math.pi:\n",
        "                 beta += 2*math.pi\n",
        "            prev_beta = beta\n",
        "            proj *= SCALE\n",
        "            if proj >  0.3:\n",
        "                 beta -= min(TRACK_TURN_RATE, abs(0.001*proj))\n",
        "            if proj < -0.3:\n",
        "                 beta += min(TRACK_TURN_RATE, abs(0.001*proj))\n",
        "            x += p1x*TRACK_DETAIL_STEP\n",
        "            y += p1y*TRACK_DETAIL_STEP\n",
        "            track.append( (alpha,prev_beta*0.5 + beta*0.5,x,y) )\n",
        "            if laps > 4:\n",
        "                 break\n",
        "            no_freeze -= 1\n",
        "            if no_freeze==0:\n",
        "                 break\n",
        "        # print \"\\n\".join([str(t) for t in enumerate(track)])\n",
        "\n",
        "        # Find closed loop range i1..i2, first loop should be ignored, second is OK\n",
        "        i1, i2 = -1, -1\n",
        "        i = len(track)\n",
        "        while True:\n",
        "            i -= 1\n",
        "            if i==0:\n",
        "                return False  # Failed\n",
        "            pass_through_start = track[i][0] > self.start_alpha and track[i-1][0] <= self.start_alpha\n",
        "            if pass_through_start and i2==-1:\n",
        "                i2 = i\n",
        "            elif pass_through_start and i1==-1:\n",
        "                i1 = i\n",
        "                break\n",
        "        if self.verbose == 1:\n",
        "            print(\"Track generation: %i..%i -> %i-tiles track\" % (i1, i2, i2-i1))\n",
        "        assert i1!=-1\n",
        "        assert i2!=-1\n",
        "\n",
        "        track = track[i1:i2-1]\n",
        "\n",
        "        first_beta = track[0][1]\n",
        "        first_perp_x = math.cos(first_beta)\n",
        "        first_perp_y = math.sin(first_beta)\n",
        "        # Length of perpendicular jump to put together head and tail\n",
        "        well_glued_together = np.sqrt(\n",
        "            np.square( first_perp_x*(track[0][2] - track[-1][2]) ) +\n",
        "            np.square( first_perp_y*(track[0][3] - track[-1][3]) ))\n",
        "        if well_glued_together > TRACK_DETAIL_STEP:\n",
        "            return False\n",
        "\n",
        "        # Red-white border on hard turns\n",
        "        border = [False]*len(track)\n",
        "        for i in range(len(track)):\n",
        "            good = True\n",
        "            oneside = 0\n",
        "            for neg in range(BORDER_MIN_COUNT):\n",
        "                beta1 = track[i-neg-0][1]\n",
        "                beta2 = track[i-neg-1][1]\n",
        "                good &= abs(beta1 - beta2) > TRACK_TURN_RATE*0.2\n",
        "                oneside += np.sign(beta1 - beta2)\n",
        "            good &= abs(oneside) == BORDER_MIN_COUNT\n",
        "            border[i] = good\n",
        "        for i in range(len(track)):\n",
        "            for neg in range(BORDER_MIN_COUNT):\n",
        "                border[i-neg] |= border[i]\n",
        "\n",
        "        # Create tiles\n",
        "        for i in range(len(track)):\n",
        "            alpha1, beta1, x1, y1 = track[i]\n",
        "            alpha2, beta2, x2, y2 = track[i-1]\n",
        "            road1_l = (x1 - TRACK_WIDTH*math.cos(beta1), y1 - TRACK_WIDTH*math.sin(beta1))\n",
        "            road1_r = (x1 + TRACK_WIDTH*math.cos(beta1), y1 + TRACK_WIDTH*math.sin(beta1))\n",
        "            road2_l = (x2 - TRACK_WIDTH*math.cos(beta2), y2 - TRACK_WIDTH*math.sin(beta2))\n",
        "            road2_r = (x2 + TRACK_WIDTH*math.cos(beta2), y2 + TRACK_WIDTH*math.sin(beta2))\n",
        "            vertices = [road1_l, road1_r, road2_r, road2_l]\n",
        "            self.fd_tile.shape.vertices = vertices\n",
        "            t = self.world.CreateStaticBody(fixtures=self.fd_tile)\n",
        "            t.userData = t\n",
        "            c = 0.01*(i%3)\n",
        "            t.color = [ROAD_COLOR[0] + c, ROAD_COLOR[1] + c, ROAD_COLOR[2] + c]\n",
        "            t.road_visited = False\n",
        "            t.road_friction = 1.0\n",
        "            t.fixtures[0].sensor = True\n",
        "            self.road_poly.append(( [road1_l, road1_r, road2_r, road2_l], t.color ))\n",
        "            self.road.append(t)\n",
        "            if border[i]:\n",
        "                side = np.sign(beta2 - beta1)\n",
        "                b1_l = (x1 + side* TRACK_WIDTH        *math.cos(beta1), y1 + side* TRACK_WIDTH        *math.sin(beta1))\n",
        "                b1_r = (x1 + side*(TRACK_WIDTH+BORDER)*math.cos(beta1), y1 + side*(TRACK_WIDTH+BORDER)*math.sin(beta1))\n",
        "                b2_l = (x2 + side* TRACK_WIDTH        *math.cos(beta2), y2 + side* TRACK_WIDTH        *math.sin(beta2))\n",
        "                b2_r = (x2 + side*(TRACK_WIDTH+BORDER)*math.cos(beta2), y2 + side*(TRACK_WIDTH+BORDER)*math.sin(beta2))\n",
        "                self.road_poly.append(( [b1_l, b1_r, b2_r, b2_l], (1,1,1) if i%2==0 else (1,0,0) ))\n",
        "        self.track = track\n",
        "        return True\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_steps = 0\n",
        "\n",
        "        self._destroy()\n",
        "        self.reward = 0.0\n",
        "        self.prev_reward = 0.0\n",
        "        self.tile_visited_count = 0\n",
        "        self.t = 0.0\n",
        "        self.road_poly = []\n",
        "\n",
        "        while True:\n",
        "            success = self._create_track()\n",
        "            if success:\n",
        "                break\n",
        "            if self.verbose == 1:\n",
        "                print(\"retry to generate track (normal if there are not many of this messages)\")\n",
        "        self.car = Car(self.world, *self.track[0][1:4])\n",
        "\n",
        "        return self.step(None)[0]\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_steps += 1\n",
        "        # car racer discrete\n",
        "        if action is not None:\n",
        "            action = self.actions[action]\n",
        "            self.car.steer(-action[0])\n",
        "            self.car.gas(action[1])\n",
        "            self.car.brake(action[2])\n",
        "\n",
        "        self.car.step(1.0/FPS)\n",
        "        self.world.Step(1.0/FPS, 6*30, 2*30)\n",
        "        self.t += 1.0/FPS\n",
        "\n",
        "        self.state = self.render(\"state_pixels\")\n",
        "\n",
        "        step_reward = 0\n",
        "        done = False\n",
        "        if action is not None: # First step without action, called from reset()\n",
        "            self.reward -= 0.1\n",
        "            # We actually don't want to count fuel spent, we want car to be faster.\n",
        "            # self.reward -=  10 * self.car.fuel_spent / ENGINE_POWER\n",
        "            self.car.fuel_spent = 0.0\n",
        "            step_reward = self.reward - self.prev_reward\n",
        "            self.prev_reward = self.reward\n",
        "            if self.tile_visited_count==len(self.track):\n",
        "                done = True\n",
        "            x, y = self.car.hull.position\n",
        "            if abs(x) > PLAYFIELD or abs(y) > PLAYFIELD:\n",
        "                done = True\n",
        "                step_reward = -100\n",
        "            if self.current_steps > self.totoal_episode_steps:\n",
        "                done = True\n",
        "        \n",
        "        if self.current_steps > self.totoal_episode_steps:\n",
        "            done = True \n",
        "\n",
        "        return self.state, step_reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        assert mode in ['human', 'state_pixels', 'rgb_array']\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(WINDOW_W, WINDOW_H)\n",
        "            self.score_label = pyglet.text.Label('0000', font_size=36,\n",
        "                x=20, y=WINDOW_H*2.5/40.00, anchor_x='left', anchor_y='center',\n",
        "                color=(255,255,255,255))\n",
        "            self.transform = rendering.Transform()\n",
        "\n",
        "        if \"t\" not in self.__dict__: return  # reset() not called yet\n",
        "\n",
        "        zoom = 0.1*SCALE*max(1-self.t, 0) + ZOOM*SCALE*min(self.t, 1)   # Animate zoom first second\n",
        "        zoom = 0.1*SCALE*0 + ZOOM*SCALE*1   # Animate zoom first second\n",
        "        zoom_state  = ZOOM*SCALE*STATE_W/WINDOW_W\n",
        "        zoom_video  = ZOOM*SCALE*VIDEO_W/WINDOW_W\n",
        "        scroll_x = self.car.hull.position[0]\n",
        "        scroll_y = self.car.hull.position[1]\n",
        "        angle = -self.car.hull.angle\n",
        "        vel = self.car.hull.linearVelocity\n",
        "        if np.linalg.norm(vel) > 0.5:\n",
        "            angle = math.atan2(vel[0], vel[1])\n",
        "        self.transform.set_scale(zoom, zoom)\n",
        "        self.transform.set_translation(\n",
        "            WINDOW_W/2 - (scroll_x*zoom*math.cos(angle) - scroll_y*zoom*math.sin(angle)),\n",
        "            WINDOW_H/4 - (scroll_x*zoom*math.sin(angle) + scroll_y*zoom*math.cos(angle)) )\n",
        "        self.transform.set_rotation(angle)\n",
        "\n",
        "        self.car.draw(self.viewer, mode!=\"state_pixels\")\n",
        "\n",
        "        arr = None\n",
        "        win = self.viewer.window\n",
        "        win.switch_to()\n",
        "        win.dispatch_events()\n",
        "\n",
        "        win.clear()\n",
        "        t = self.transform\n",
        "        if mode=='rgb_array':\n",
        "            VP_W = VIDEO_W\n",
        "            VP_H = VIDEO_H\n",
        "        elif mode == 'state_pixels':\n",
        "            VP_W = STATE_W\n",
        "            VP_H = STATE_H\n",
        "        else:\n",
        "            pixel_scale = 1\n",
        "            if hasattr(win.context, '_nscontext'):\n",
        "                pixel_scale = win.context._nscontext.view().backingScaleFactor()  # pylint: disable=protected-access\n",
        "            VP_W = int(pixel_scale * WINDOW_W)\n",
        "            VP_H = int(pixel_scale * WINDOW_H)\n",
        "\n",
        "        gl.glViewport(0, 0, VP_W, VP_H)\n",
        "        t.enable()\n",
        "        self.render_road()\n",
        "        for geom in self.viewer.onetime_geoms:\n",
        "            geom.render()\n",
        "        self.viewer.onetime_geoms = []\n",
        "        t.disable()\n",
        "        self.render_indicators(WINDOW_W, WINDOW_H)\n",
        "\n",
        "        if mode == 'human':\n",
        "            win.flip()\n",
        "            return self.viewer.isopen\n",
        "\n",
        "        image_data = pyglet.image.get_buffer_manager().get_color_buffer().get_image_data()\n",
        "        arr = np.fromstring(image_data.get_data(), dtype=np.uint8, sep='')\n",
        "        arr = arr.reshape(VP_H, VP_W, 4)\n",
        "        arr = arr[::-1, :, 0:3]\n",
        "\n",
        "        return arr\n",
        "\n",
        "    def close(self):\n",
        "        if self.viewer is not None:\n",
        "            self.viewer.close()\n",
        "            self.viewer = None\n",
        "\n",
        "    def render_road(self):\n",
        "        gl.glBegin(gl.GL_QUADS)\n",
        "        gl.glColor4f(0.4, 0.8, 0.4, 1.0)\n",
        "        gl.glVertex3f(-PLAYFIELD, +PLAYFIELD, 0)\n",
        "        gl.glVertex3f(+PLAYFIELD, +PLAYFIELD, 0)\n",
        "        gl.glVertex3f(+PLAYFIELD, -PLAYFIELD, 0)\n",
        "        gl.glVertex3f(-PLAYFIELD, -PLAYFIELD, 0)\n",
        "        gl.glColor4f(0.4, 0.9, 0.4, 1.0)\n",
        "        k = PLAYFIELD/20.0\n",
        "        for x in range(-20, 20, 2):\n",
        "            for y in range(-20, 20, 2):\n",
        "                gl.glVertex3f(k*x + k, k*y + 0, 0)\n",
        "                gl.glVertex3f(k*x + 0, k*y + 0, 0)\n",
        "                gl.glVertex3f(k*x + 0, k*y + k, 0)\n",
        "                gl.glVertex3f(k*x + k, k*y + k, 0)\n",
        "        for poly, color in self.road_poly:\n",
        "            gl.glColor4f(color[0], color[1], color[2], 1)\n",
        "            for p in poly:\n",
        "                gl.glVertex3f(p[0], p[1], 0)\n",
        "        gl.glEnd()\n",
        "\n",
        "    def render_indicators(self, W, H):\n",
        "        gl.glBegin(gl.GL_QUADS)\n",
        "        s = W/40.0\n",
        "        h = H/40.0\n",
        "        gl.glColor4f(0,0,0,1)\n",
        "        gl.glVertex3f(W, 0, 0)\n",
        "        gl.glVertex3f(W, 5*h, 0)\n",
        "        gl.glVertex3f(0, 5*h, 0)\n",
        "        gl.glVertex3f(0, 0, 0)\n",
        "        def vertical_ind(place, val, color):\n",
        "            gl.glColor4f(color[0], color[1], color[2], 1)\n",
        "            gl.glVertex3f((place+0)*s, h + h*val, 0)\n",
        "            gl.glVertex3f((place+1)*s, h + h*val, 0)\n",
        "            gl.glVertex3f((place+1)*s, h, 0)\n",
        "            gl.glVertex3f((place+0)*s, h, 0)\n",
        "        def horiz_ind(place, val, color):\n",
        "            gl.glColor4f(color[0], color[1], color[2], 1)\n",
        "            gl.glVertex3f((place+0)*s, 4*h , 0)\n",
        "            gl.glVertex3f((place+val)*s, 4*h, 0)\n",
        "            gl.glVertex3f((place+val)*s, 2*h, 0)\n",
        "            gl.glVertex3f((place+0)*s, 2*h, 0)\n",
        "        true_speed = np.sqrt(np.square(self.car.hull.linearVelocity[0]) + np.square(self.car.hull.linearVelocity[1]))\n",
        "        vertical_ind(5, 0.02*true_speed, (1,1,1))\n",
        "        vertical_ind(7, 0.01*self.car.wheels[0].omega, (0.0,0,1)) # ABS sensors\n",
        "        vertical_ind(8, 0.01*self.car.wheels[1].omega, (0.0,0,1))\n",
        "        vertical_ind(9, 0.01*self.car.wheels[2].omega, (0.2,0,1))\n",
        "        vertical_ind(10,0.01*self.car.wheels[3].omega, (0.2,0,1))\n",
        "        horiz_ind(20, -10.0*self.car.wheels[0].joint.angle, (0,1,0))\n",
        "        horiz_ind(30, -0.8*self.car.hull.angularVelocity, (1,0,0))\n",
        "        gl.glEnd()\n",
        "        self.score_label.text = \"%04i\" % self.reward\n",
        "        self.score_label.draw()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c61fq6FMVv0A"
      },
      "source": [
        "## TEST CAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-3RPjFbySdb6",
        "outputId": "752ffb36-254d-47fc-a45b-79231cc9baba"
      },
      "source": [
        "# from gym.envs.box2d import CarRacing\n",
        "env = gnwrapper.Animation(CarRacing())\n",
        "env = CarRacing()\n",
        "\n",
        "env.reset()\n",
        "env.render()\n",
        "im = env.render(\"state_pixels\")\n",
        "\n",
        "def state_image_preprocess(state_image):\n",
        "    # crop image\n",
        "    state_image = state_image[0:84, :, :]\n",
        "    state_image = state_image.transpose((2,0,1))\n",
        "    # to torch\n",
        "    state_image = np.ascontiguousarray(state_image, dtype=np.float32) / 255\n",
        "    state_image = torch.from_numpy(state_image)\n",
        "    return state_image.unsqueeze(0).to(device)\n",
        "\n",
        "# plt.imshow(im)\n",
        "state_image_preprocess(im).shape\n",
        "plt.imshow(state_image_preprocess(im).cpu().squeeze(0).permute(1, 2, 0).numpy())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Track generation: 1232..1552 -> 320-tiles track\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:427: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc8762c2278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARoAAAD7CAYAAAC13FspAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAei0lEQVR4nO2da4wk11XHf6fnsTOesb32etksu34sibWRFcV2WIyjRAhighwSxVGEQgJEKASZDzwSHiKGLwgJJCKhPD4gxMoJ+EPIg+CIKIpMLMcRICFjO04gsWO8dux4N+td43jX9u7O7Mz04cOt3unp6e669bhVt6rOT2rNdFdX31tVp/517uscUVUMwzBC0qu7AoZhtB8TGsMwgmNCYxhGcExoDMMIjgmNYRjBMaExDCM4hYRGRG4VkcdF5IiI3FFWpQzDaBeSdx6NiMwA/wu8FTgKPAi8T1UfLa96hmG0gdkC+94EHFHVpwBE5HPAbcBEoVm4fEEvvvLiAkV6oMBG8tfwZxXkjEC/7oqUTA90SWFH3RVpPy//8GVWTq3IuG1FhGYf8OzQ+6PAT0/b4eIrL+bd97y7QJETUGBweKvAS8Ba+cW0FgV5Sph5cAY5t2knqorIWLvJXkSO3yqjfF1SNm7aQK/WTRsxgnD3+++euK2I0HghIrcDtwMs71sOVMjQ/zPAIjCfso8C54H1MFVqA2WJzPBvZRGPMss36qWI0BwDrhx6vz/5bAuqehg4DLD7+t3hGzQzwEUe3+sDL2NCUzEmHt2kiNA8CFwrIgdwAvNe4FdKqVURZOTvNGaSVxpK+/ouPCmz+WR0l9xCo6rrIvI7wL/ibtdPq+p3S6tZEXzuC2GziZXmZ60C5zy+10KGRcZEx8hLoT4aVf0q8NWS6lItAswlr2kMvJkVOiE046Y7DMQlr8j4CJSJWLsJ3hkcFcOjU1mYBRZIbz4pbrSroc0sRZGSh2Z8BaTw6JIJVdTEIzRZRGD0u7775rXDOdyZSvNo1nEdzE0UGk28mZFjLHrzVnXzm8jETTxCM85OJgnI6GchbUwy/L7iFnX4fL9BzbCmRGE0sYmXeIRmHFnsJm+zqOi+w/RwQ+sLHuWt4ObxRE4TRMYEJn7iFposFLG1suy0R7rIwOYyiQYIjWGUQRxCU5ZHUXd5ab/Zx/Xj9GlMP86kTtaYOl8HdRnnfWmT2qgtJg6hqdpeyygvj1itAWfYFJsGMElM8ojMsCCULVKTmnjjOriN6olDaLrCYPh7o+6K1EPR+Th5Ma+mftoTYa+ILeXZN45Wg2E0gvYITQydwYZhjKV+oZnmTZjH2xmaMIxu5Kd+oZnmTTTB00i7Pzp6/4wdAZoiJrGMYBlhqF9o2k7O+6fpT/hxwlGGmDT9vDSdvOc/XqFJOZ4LIwk12N2WUYxAD+K2POHLFoZJ5yUWAaqzHlWUndcu4xWalOO5sMp49Ht5mjLqsd+4so1Uuraoss56xHIOxhGv0OQl7VxPWqQZ7zVqBbF4HEY9tE9opmG2XhsxP22N8KQKjYh8WkROish3hj67XETuFZEnkr+Xha1mSYSwdRMvw0jFx6P5B+DWkc/uAO5T1WuB+5L3zaIMgah6MWhAFL0Q/CrryzDSSBUaVf034EcjH98G3JX8fxfwrpLrFZ4iAjG4t4RWeTT9fj/Ta1RkTHSMSeRdVLlHVY8n/z8H7CmpPs1AJvw/Dp3wf2wUqNuwwEQnNpFVp6sUXr2tqioiEy9nJZkqY0Zx6VrWaVR4CB+iE5VxNKCKXSCv0JwQkb2qelxE9gInJ30xWKbKuvpHppU7btsgbGdL07VELzYZ50gZYcg7vP1l4NeT/38d+JdyqjPCNAOpqxM2z9osM3aj4/gMb38W+E/goIgcFZEPAn8FvFVEngB+PnlfPi3rbDWMrpLadFLV903YdEvJdRlTOFM9iDwJzxRFVMLNqSnpd6uOyZvrXEYUN3gaSsXnMsd5acy5zFnPeGcGe9y0gmT2eISSRWa4/BJ/t2qjy7N+qwk3BgKyQzbzrFdQ5TznpRHnkg4uqsz8vVDUXb4xnR5OZC5J/tr1qgULTu5Di2YAd5Je8ppJXmlY533pmND4YCLTDmaBJdJFpA+cw817MkrBhMboDjO45lMagyyiJjSlYUJjdIcs/X5z+DWfNuhsnq4sNFNoyugzsX6X7uF7zXvARaR7P4prYp0pWK8O0EyhKUMgTGS6RxaPxufOUFwTyyaWphK30JjXYcTIsF3OAcukC80GbnFtixbVZiFuoTGRMWJk2C7nk1caq7i86yY0HWaS55TXoxrMw+jjnmRNcKstQHt2spyvHtvvtoGNNME+ChKn0FTdZJpUVpE6nAfO4gypAcOkIkKv1/Ob0ObJuCh8UTCwr6x2VsQuZ3FNrGGPZgNnI2s5f7NBxCM0wxexDU/WQZs8wvtsEiKC9Mo5+YPFd1EKTRY7K8sue2xvYq3h4hR1gPqEZvTpULW4lO01Wcf1NkSknsWCAlwBukfhYmBnhn3rtsuWEo9Hk4c8N/dgn7INaPB7ET7A60JEmJkpsS3mXTD0r+6jN2v6qu0QDwh76Gyj2UJjREvVnsxwnBQVdcPOS8ACJv4REG+YCB/y2HIoz2M4BYtxgar6aKYKW57wq0UwG9iGTyjPK0XkfhF5VES+KyIfSj4vlq0y5MXwse1QTSdjC9EHdBq1lbrtsqX4eDTrwB+q6nXAzcBvi8h1hMxWWfSCRG7bbaeIF5Nn38z76NDfrMPbReiwXfpkqjyuqt9M/n8ZeAzYR8hslXWMQGX53JhKES8mz5B45vLyDld3WCiKkqmPRkSuAW4EHsAzW6WI3C4iD4nIQysvRDppIMSEPSM3WYQjynk6xja8hUZEloF/Bj6sqi8Nb1N3tcdecVU9rKqHVPXQwq6FbLUzGzJSqKUPyOwyM15CIyJzOJH5jKrenXx8IslSSVq2ytyYR9EaWuV5mF1mxmfUSYBPAY+p6seGNlWTrTKNFtlvm8nreTRWoBpa7VD4TNh7E/B+4H9E5FvJZ3+Ky075hSRz5TPAe8JUMQV7ukRN0cRo0Q+PT6Kh1Q6FT6bK/2DyaQufrdJoNI0VilB01NOxJQi+2PqVqGlKSlnAzUxbo1OBzU1ofDCRiZ6JIhPj0pBVXEDzjgS9ghiEJu0mzmooIUQhS9nD0fVip8/mk/U8oPV0vpbpiahqOWEeQtql0hlPZkD9QpO2fD/tQtYdP2S4/A02MxyuE//TagXkSYEfgbws6HmttM4hmjoigpZxEE23y8ioX2gm4RFqUcneLlcUUSnvwg//juLc4vPFf7aSPoc1kOMCz4YtJi9Zz8EFb2xkF0WRsi54KLtsSB9T3nrGJzQZQifmMR5hgshE1g9TmdFpvvOYq6gRI/UJ95mrKTcyT72U4wttlw0QGchfz/iEZtRDqOr8N+M6N5pxRhqqT6iU5tMwddllS4g78JVPZ1zVxN7v0nJ8hGmiyJR17WK0y8iJU2jSLladTxR7ko0likh6qTsXLDxmu4ycOIUm7WLZxYyCYXG5EK+3qWuTfDC7zE2cQhM7OuH/jjHqXTRl5MSonjiFJu/Nm2W/It8dnnjV8ftqnFfTWqqwy5YSp9Dktdcs+5Xx3cGcig7TSHHJe82qsMuWEqfQhCKEKJgRNQ+7ZpXTLaExAzOMWohvwl5RfBbDjW6PcYWv0S4GC22bsuC2ZNonNL5DkBmmlI9S6toZoxSiH/FaZ+uC2wDEfA58YgYviMh/ici3k0yVf558fkBEHhCRIyLyeRGZD1/dEik072to50Cdwa2ej1KASedl2g1W+nKEPPSBleQVSGiqEJm8dunTR7MKvEVVrwduAG4VkZuBjwIfV9XXAC8CH8xVgzawhjOgVUpzi2N9MvkyziDLEM8858W8z/LIa5c+mSpVVV9J3s4lLwXeAnwx+fwuysxU2TRWgZdwUdMCPa2axjiDLCqe5uU1F68+GhGZAR4GXgP8DfAkcEpVB7fVUVya3HH73g7cDrC8b3n7E98niFDMDDr3GiwwMd/Aw+LUdC+vy3gJjapuADeIyE7gS8BrfQtQ1cPAYYDdr9utvDK0UYAFnI9UlCKzdDs8w7evfRcILEJaISwdtq1hMo06qeopEbkfeCOwU0RmE69mP3As9Qf6sEVoZthsjBWlyMXssiHUFCfYh1YITQsOoQx8Rp12J54MIrIIvBV4DLgf+KXka/kyVSquyXHe47VBXNP9Y6pLS1HVXK/RCHvTCwl5BMYAH49mL3BX0k/TA76gql8RkUeBz4nIXwCP4NLmZqMPnMWN2KSxCFyUuYTxlOHO2pMqOBeEIys9/AUk6+ROIxc+mSr/G7hxzOdPATcVroFvJ+o82Z4+0wykiPGY8cXPJDvJeu3SMiEU/f0O0ZyZwWs47yftQvaAHR7fGyaLgZghNZcQmS/SPjfxAZokNL5pTOZxR5VluagZQrcoevPbgykzzREa8Gs69XEdxz5CI0MvoztY/1zlNEtofNjAzdD1aWIt4jygvNiIRfxY0yUK2ic0fVwzK40emyKT1xgtwl78mMhEQfuExhfFdTD7hJWYwZ0pM1ojC4N5Yhs4W+vwQ6nbQnMO/zk8y5jQGNkY2Ng5OhvwakB3hQb8L/zGlO9mmYVqdIuBuGzUXZH66bbQ+LKGW6M1yaPxGXY3jA5jQuNDwPCLRs3YqFQlmNB0Hdm6SjrWldylUiBetNdvGtswoek4vV4P6W29Q/r9frsFJ4QgmMhMxYSm44gIvd7mNOpBJP1WC800LIBaEExojG2Mik+M5A4hkca4nF+2rqkwJjTGNkQk+uh2qsrGRgXjxnGfhsZgQmNsITaBmZQUbWoTL65DMGiw0NSZlS/mjIB5iPl4JtVravNOoN/rT08cF6g/pc4spjFfR++GuIjMiMgjIvKV5H2tmSrrPKGxXsy8jB5Plr6PujqNB827sa+eR9Mv0CW8IDI1nJaY7TJLj9+HcEHJB1imypbiY7ADgZn03SoEaFhcht8HpYxYxB3ES2hEZD/wduDO5L1gmSo7TdoNXdYNP02whrdV9jQ3AcmFr0fzCeCP2VxauIsMmSpF5CEReWjlRZ+l0oaxyTQBibmpYGzFJ6/TO4CTqvpwngJU9bCqHlLVQwuXLeT5CcMoRkfnHsaEz6jTm4B3isgv4hLYXgJ8kjyZKo1WkHV0o/bRkFAZMcrct+WkejSq+iequl9VrwHeC3xdVX+VMjJVGlHh24GbVTSmfT+tzMpHtYqmVlZc/JnzuPAiHQ52NUyReeYfAf5ARI7g+myyZ6o0oqIOr6PMTuXcolS2lq0BLycvi1UEZJywp6rfAL6R/F9OpkojWsps8uT5rcz7CKioe3z67haiudPHvJkRGjsz2AhPmR7O4LeyiEfm8pdAr1b0EkX3qwsqn1pItiKMfJjQGJUStHm2DP3r++i+RGTShMY6byvDhMbI1bcR4xwWFXW5unZ47hDfIbQWE5qOo6qZO0NjFJmxmMcSDSY0XSfvQE0TIvCZyERD3GHUjErIKhoxi8yFuunQy6gd82iMXE2hGMVmS53qyG4Q3ymJBhMaoz0MPJhp81eyplkZ971J4jOYGWxsw4TGaA994Azw0oTtc7g86kU7DKzvJzMmNEZ76AMrwNkJ2xdwQ9/WM1k5JjTGVNKCgzeKwdKAtOQJgrszBPNeSsKExpjKtODgWRnOXFDLGqrBYse0r/eAZfwn/hmpmNAYQDUeynBs37J/0wsF1lO/5ZYu9LGO3RIxoTGABs32rYJBX0+aKAmug3keJ0qryT7rmEiNYEJjGKMocA6//pklnNj0k31WMZEZgwmNEQVRdi77CEYf17ncx5pbUzChMYIwTjimiUl0IuPLKk5ofPt/OoqX0IjI07j++g1gXVUPicjlwOeBa4Cngfeo6othqtk9gj7hK3jqjqt7GcdTSaS+LGyQPlzeIvKeyyxTl35OVW9Q1UPJ+zuA+1T1WuC+5H1l1LnWpqosjEFQ4BTwNMizMnlyW1nFlXyu8mTGrNJbMrscT5E5krfhMlRCDZkqLfd2ThTkuNB7qId8W+B02OKqOlexXBOzy/H49tEo8DURUeDvVPUwsEdVjyfbnwP2jNtRRG4HbgdYftVyweoahVHcxLWzyV9AA7SlsvTPGO3HV2jerKrHROTHgHtF5HvDG1VVExHaRiJKhwF2X7fb+uQjQFWDjpCE6p8xmouX0KjqseTvSRH5Ei7NygkR2auqx0VkL3AyYD2NkskTwtMXExVjFJ/c20sicvHgf+AXgO8AX8ZlqATLVGkMoaq1vYw48fFo9gBfSp5Ss8A/quo9IvIg8AUR+SDwDPCecNU0mkRtN72F7oyWVKFJMlJeP+bzF4BbQlTKMHJhIhMtFgLIMIzgmNAYhhEcExrDMIJjQmMYRnAaLTR5RjaaMgRadT3tXNZbXtvPZaOFJs/EsKZMJqu6nnYu6y2v7eey0UJjGEYzMKExDCM4JjSGYQTHhMYwjOCY0BiGERwTGsMwgmNCYxhGcCzdSgcREaQntTxmLG5MNzGh6SAiQm+m58J5Vky/3zeh6SAmNB2m6tmogwDlIhJGbJoxubaTmNAYpZOWkbLXC9Nm057SlxrcNCMV30yVO4E7gdfh4pj9BvA4lqnSGENa2tuyPKlRQetLH0GCpI8xiuH7aPkkcI+qvhYX1vMxas5UaZRHyD6TkL/dlIWIhl8WhEuBnwE+BaCq51X1FDVnqjTKI+QNW5UYWAdz3Ph4NAeA54G/F5FHROTOJO2Kd6ZKEXlIRB5aeXGlnFobhtEofIRmFngD8LeqeiNwhpFmkrrHycRMlap6SFUPLVy2ULS+RgCa2nQaxppRceMjNEeBo6r6QPL+izjhOZFkqMQyVTabNjSdjLhJFRpVfQ54VkQOJh/dAjyKZao0JlDEi2lzGMwu4zuP5neBz4jIPPAU8AGcSFmmSmMbRbyYwWS+LL9hXlP8eAmNqn4LODRmk2WqNEoni3BkFSWjHmz1ttFoTGSagQmNcYE2jD4ZcVLtWicZKVFxK4jNBmthtNkR4+iTNY3aQbVCMwNcOvR+AzcrZ63SWhgJVdzARYXCRKYdVO/R7Bh6vw6sYEJTE1U1Z/KUYwLTLuoNEzEQnrSeIsWJkglSKagq/X4f0fhu5iJNLCNe6hWaHrAI+KxMOIMTG7Onwqgq2lfXdI2MXq+XK16NiIWHiJn6PRqY7tHo0Hd6pAvNxFVXxhYiP0fmobSL+iPsTfKUNdkmyf8+TSxwXs85onxaG+kUiincx7yaSKlfaCYxEJiBEM2yrbaKIqNKdR7XwTyBpgyXVl3PPOWFqmMRoRmnM10+l2WTt57xCc2wuIz+HWGbyIDzeuZxQ+nT9hl0MEcaYrZqo8tT3mCf3sYGl54+zdKZM6n7nFla4vSll9KfmXCBAtCkcxk7eesZn9AMH4cyuWk1iRlgGb++nFeY6v0YfsytrfGaI0e45umnkSneiIrw/QMH+O/Xv57zFQqNUT/xCc0w00RmkggNOo3T2Ei+N2iiGbkRVWbPnWPh1Cl6wBxbL0EfNzOhD8ydPYv0I3UjjWDEKTRpnkweT2cUwQ2rz+CaUKtE24yKnXPAfwD/DuwH3gxcPrT9xWT7MdxpH+i70R3iFJo0KyzDSgeTBXfgRGbwyDUyswI8DDwB/BQuJ8+w0JwC7k2+cy0u3ogFde0WcQpNFdgjtVQ2cFq9wfaW6PDE7nHbC3MRTrkuxbXbjOjwSbdyUES+NfR6SUQ+LCKXi8i9IvJE8vey0mqV1xKz7Gf9Mu1gBrgaeCPweuDieqtjjMcnZvDjqnqDqt4A/CRwFvgSIRPI5fU2suxnHk3pTBv6lGR76cO4AlwC7AV246Y2GNGRtel0C/Ckqj4jIrcBP5t8fhfwDeAj5VUtAGV0IhvbmJmZYe/evfR6Pa46c4YdJ07A6uqF7Tt27OCqPXt4ZWmJV73qVczOdrfF3lWyXvH3Ap9N/vdKIBcVJjJBmJub48CBA1x11VVc+cMfsnj69BahWVxc5ODBgyzt3cvMzIwJTQfxXiabZEB4J/BPo9umJZDbkqnyhQpmx/lM1DNKRUSYm5tjYWGB+fl5ZGT1dU+E+fl5FhYWmJuba8wsWKM8sqzHfxvwTVU9kbz3SiC3JVPlrgoGNX2HxgsIjq0sLg87l+UR87nMIjTvY7PZBE1PIFfgoVpVCMwuYOeyPGI+l16NZRFZAt4K/NbQx3+FJZALRpOaF72NDXaeOsXyK6+w64UXmFvbGgpxbm2NHzt5ktn1dV5ZXubUzp2tXlTZZoIuqlTVM8Cukc9ewBLIGTghefWTT3Lg+99nbm2NhZWtfXEXnT3LdY8+yvrs7IVFlau2qLJTWPe/URhRZfHcOS49fXpsi3Sm32f5zBkUWDx3buoKb6OdtCeBnNmuYURLe4TGmuGGES3dbTopboVfn0Ar/brDBm7GpgJLuJmbw+m7VoETuEQWz+EWWBrdIg6hqXppwGB64Tk249BYiIjcnAXuAZ4HrsNNH983tP154PPA93DLka7EFll3jTiEJi0Tgu/nWcrr4x6t5wv8TpPQob8le2/rwLPA47hoDaPzv1eAJ4FvAwdx6x9NaLpFHEIziUliEkJ82s5ZXLvlLG4Ot3lwRoXELTRZxMNEZjqvAI8CL7DZN2UYFRG30JQpHl33eAYRwgM0FXu4kDB7gJ1sN6pZ4LJk+yW0aajT8CVuoSmT0YR0Rmks4tanXIyLFbxrZPsVwC8BbwFeBn6EOVRdoztCAyYygZjHjTYdnLB9CXhD8v/jwH9h6bS6RreExghCv9fjhV27eObqq1O/+39XXMGGrXPqHO0UGmsiVcra3BxPvvrV/OCqq7y+u24R9jpHO6+4iUylaK/HyuIiK4uLdVfFiBQbADAMIzjtEBpbp2QYUdO8ptO4/hdrKnUXwSWRmyP9sTlYQGtUjm8oz98HfhN3m/8P8AHckpXP4aZNPAy8X1XDrxwyUTFGGaTDTROaVdwScpvEUzk+KXH3Ab8HHFLV1+GeH+8FPgp8XFVfA7wIfDBTyUWaO3lTqihBFhUaNTLs0cynvGaxB1VN+PbRzAKLIjKLS6l+HDfR84vJ9ruAd2UqefSCZ7n5fVOqjGMdt7DwLBYYpWsMrHcp5bWIEy+jNFKbTqp6TET+GvgBLoLL13BNpVOqOrhVj7I1BEl2qnrSrOEWGPYxz6ZrzOKEJI0N4CWsP6dEfJpOlwG3AQeAH8ddqlt9C6g8U6UP1nxqL9Ouq+AsPu0lWBOrZHyaTj8PfF9Vn1fVNeBu4E3AzqQpBbAfODZu51CZKrVGpWhbQrI6j6f0sn0FItAht+pcloiP0PwAuFlELhKXPeoWXGST+3GLcqGGTJUysKgazm3bEpLVeTy1lR2o2E6eSw9ShUZVH8B1+n4TN7TdAw4DHwH+QESO4Ia4P1VqzXwFJN5za9RFvA/2zuKbqfLPgD8b+fgp4KbSazTABMTIgy2ojZJ2LEEwjAEmMlFiQmO0H2tK1U59QlPHzGCjm2Txcsx2glCf0BTNyxTqt41uY7YTBKly7F1Enscta/u/ygoNzxXY8cRKm44F4j+eq1V197gNlQoNgIg8pKqHKi00IHY88dKmY4FmH491BhuGERwTGsMwglOH0ByuocyQ2PHES5uOBRp8PJX30RiG0T2s6WQYRnAqFRoRuVVEHheRIyJyR5VlF0VErhSR+0XkURH5roh8KPn8chG5V0SeSP5eVnddsyAiMyLyiIh8JXl/QEQeSK7R50Vkvu46+iIiO0XkiyLyPRF5TETe2OTrIyK/n9jad0TksyKy0NTrU5nQiMgM8DfA23Cpmt8nItdVVX4JrAN/qKrXATcDv53U/w7gPlW9Frgved8kPgQ8NvS+WCzoevkkcI+qvha4Hndcjbw+wWJ110SVHs1NwBFVfSrJlvA5XOS+RqCqx1X1m8n/L+OMeB/uGO5KvpY9dnKNiMh+4O3Ancl7oWgs6JoQkUuBnyEJV6Kq51X1FA2+PoSI1V0TVQrNPuDZoffF4wzXhIhcA9wIPADsUdXjyabngD01VSsPnwD+mM0EJLsoOxZ0dRwAngf+PmkK3ikiSzT0+qjqMWAQq/s4cJoQsborwjqDMyIiy8A/Ax9W1ZeGt6kbwmvEMJ6IvAM4qaoP112XkpgF3gD8rareiFvqsqWZ1LDrUyhWd2xUKTTHgCuH3k+MMxwrIjKHE5nPqOrdyccnRGRvsn0vcLKu+mXkTcA7ReRpXDP2Lbg+Dq9Y0BFyFDiaRIQE17x4A829PoVidcdGlULzIHBt0ms+j+vY+nKF5Rci6b/4FPCYqn5saNOXcTGToYbYyXlR1T9R1f2qeg3uWnxdVX+VmmNB50VVnwOeFZGDyUeD2NaNvD5EGqs7L1Wv3v5FXL/ADPBpVf3LygoviIi8Gfh3XNzkQZ/Gn+L6ab4AXAU8A7xHVX9USyVzIiI/C/yRqr5DRH4C5+FcDjwC/JqqrtZZP19E5AZcx/Y8LtTsB3AP00ZeHxH5c+CXcSOej+DSUu+jgdfHZgYbhhEc6ww2DCM4JjSGYQTHhMYwjOCY0BiGERwTGsMwgmNCYxhGcExoDMMIjgmNYRjB+X/aj9ikg52PGAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVLHsrOIcRXO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "472cdcff-b556-4b82-85c0-56f04cff4d72"
      },
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "\n",
        "def get_cart_location(screen_width):\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "# def get_screen():\n",
        "#     # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
        "#     # such as 800x1200x3. Transpose it into torch order (CHW).\n",
        "#     state_image = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
        "#     state_image = state_image[0:84, :, :]\n",
        "#     state_image = state_image.transpose((2,0,1))\n",
        "#     # to torch\n",
        "#     state_image = np.ascontiguousarray(state_image, dtype=np.float32) / 255\n",
        "#     state_image = torch.from_numpy(state_image)\n",
        "#     return resize(state_image).unsqueeze(0).to(device)\n",
        "#     # Cart is in the lower half, so strip off the top and bottom of the screen\n",
        "#     _, screen_height, screen_width = screen.shape\n",
        "#     screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "#     view_width = int(screen_width * 0.6)\n",
        "#     cart_location = get_cart_location(screen_width)\n",
        "#     if cart_location < view_width // 2:\n",
        "#         slice_range = slice(view_width)\n",
        "#     elif cart_location > (screen_width - view_width // 2):\n",
        "#         slice_range = slice(-view_width, None)\n",
        "#     else:\n",
        "#         slice_range = slice(cart_location - view_width // 2,\n",
        "#                             cart_location + view_width // 2)\n",
        "#     # Strip off the edges, so that we have a square image centered on a cart\n",
        "#     screen = screen[:, :, slice_range]\n",
        "#     # Convert to float, rescale, convert to torch tensor\n",
        "#     # (this doesn't require a copy)\n",
        "#     screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "#     screen = torch.from_numpy(screen)\n",
        "#     # Resize, and add a batch dimension (BCHW)\n",
        "#     return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "def get_screen():\n",
        "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
        "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
        "    state_image = env.render(mode='state_pixels')\n",
        "    state_image = state_image[0:84, :, :]\n",
        "    state_image = state_image.transpose((2,0,1))\n",
        "    # to torch\n",
        "    state_image = np.ascontiguousarray(state_image, dtype=np.float32) / 255\n",
        "    state_image = torch.from_numpy(state_image)\n",
        "    return state_image.unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
        "    _, screen_height, screen_width = screen.shape\n",
        "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
        "    view_width = int(screen_width * 0.6)\n",
        "    cart_location = get_cart_location(screen_width)\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # Strip off the edges, so that we have a square image centered on a cart\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # Convert to float, rescale, convert to torch tensor\n",
        "    # (this doesn't require a copy)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize(screen).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy())\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Track generation: 1101..1380 -> 279-tiles track\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:427: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES2-Xj9PcRXQ"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3xN6Mr8oVHm"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYpObetVgNWH"
      },
      "source": [
        "## TRain old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf1pNJBocotM",
        "outputId": "7e0fb931-e652-4c8d-da5a-4474022ded69"
      },
      "source": [
        "env.actions"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-1.,  0.,  0.], dtype=float32),\n",
              " array([1., 0., 0.], dtype=float32),\n",
              " array([0., 1., 0.], dtype=float32),\n",
              " array([0. , 0. , 0.8], dtype=float32),\n",
              " array([0., 0., 0.], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2U5U_IKcRXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44af0c94-057f-4576-cd14-8978915eaca1"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "# Get screen size so that we can initialize layers correctly based on shape\n",
        "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
        "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
        "init_screen = get_screen()\n",
        "_, _, screen_height, screen_width = init_screen.shape\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "# n_actions = env.action_space.n\n",
        "n_actions = len(env.actions)\n",
        "\n",
        "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations():\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:413: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPxVE3_w276K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15bfb1d2-aefd-4078-e344-f8063f7b4f28"
      },
      "source": [
        "memory.position"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QGYM9yBcRXT"
      },
      "source": [
        "Training loop\n",
        "^^^^^^^^^^^^^\n",
        "\n",
        "Finally, the code for training our model.\n",
        "\n",
        "Here, you can find an ``optimize_model`` function that performs a\n",
        "single step of the optimization. It first samples a batch, concatenates\n",
        "all the tensors into a single one, computes $Q(s_t, a_t)$ and\n",
        "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our\n",
        "loss. By defition we set $V(s) = 0$ if $s$ is a terminal\n",
        "state. We also use a target network to compute $V(s_{t+1})$ for\n",
        "added stability. The target network has its weights kept frozen most of\n",
        "the time, but is updated with the policy network's weights every so often.\n",
        "This is usually a set number of steps but we shall use episodes for\n",
        "simplicity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaxGRO2ScRXU"
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    debug=False\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    \n",
        "    if debug:\n",
        "        print(\"action_batch\", action_batch.size())\n",
        "        print(\"reward_batch\", reward_batch.size())\n",
        "        print(\"state_batch\", state_batch.size())\n",
        "        print(\"state_action_values\", state_action_values.size())\n",
        "        print(\"non_final_next_states\", non_final_next_states.size())\n",
        "        print(\"expected_state_action_values\", expected_state_action_values.size())\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8BQ4jUiI3y2"
      },
      "source": [
        "optimize_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU17NcQmcRXV"
      },
      "source": [
        "Below, you can find the main training loop. At the beginning we reset\n",
        "the environment and initialize the ``state`` Tensor. Then, we sample\n",
        "an action, execute it, observe the next screen and the reward (always\n",
        "1), and optimize our model once. When the episode ends (our model\n",
        "fails), we restart the loop.\n",
        "\n",
        "Below, `num_episodes` is set small. You should download\n",
        "the notebook and run lot more epsiodes, such as 300+ for meaningful\n",
        "duration improvements.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7DH_e8hcRXV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "outputId": "65659aa1-e040-4dc4-d7d7-256f79467748"
      },
      "source": [
        "\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(30)\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import cv2\n",
        "import base64\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "# import wandb\n",
        "import wandb\n",
        "import time\n",
        "\n",
        "# record gameplay video\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()\n",
        "\n",
        "DEBUG               = False\n",
        "NUM_OF_EPISODES     = 2\n",
        "NUM_OF_STEPS        = 100\n",
        "LOG_VIDEO_EPISODES  = 2\n",
        "WANDB_ID            = \"sexyid123_new2\"\n",
        "WNDB_NAME           = \"Test_bruuuu_new_new2\"\n",
        "LOAD_SAVED_MODEL    = False\n",
        "MODEL_SAVE_NAME     = \"DQN_test_model\"\n",
        "OPTIMIZER_SAVE_NAME = \"DQN_test_optimizer\"\n",
        "SAVED_MODEL_VERSION = \"latest\"\n",
        "\n",
        "\n",
        "# wnadb api key\n",
        "# 00d5bfbd342bb73d5aaf4f2833436d20457ef040\n",
        "os.environ[\"WANDB_ENTITY\"]  = \"andreas_giannoutsos\"\n",
        "os.environ[\"WANDB_PROJECT\"] = \"gym_car_racer\"\n",
        "os.environ[\"WANDB_RESUME\"]  = \"allow\"\n",
        "wandb.init(resume=WANDB_ID)\n",
        "wandb.run.name = WNDB_NAME\n",
        "\n",
        "# load model\n",
        "if LOAD_SAVED_MODEL:\n",
        "    try:\n",
        "        model_artifact = wandb.use_artifact(MODEL_SAVE_NAME+':'+SAVED_MODEL_VERSION, type='model')\n",
        "        artifact_dir = model_artifact.download()\n",
        "        saved_model = torch.load(artifact_dir+\"/\"+MODEL_SAVE_NAME+\".pth\")\n",
        "        saved_optimizer = torch.load(artifact_dir+\"/\"+OPTIMIZER_SAVE_NAME+\".pth\")\n",
        "    except:\n",
        "        print(\"no model found\")\n",
        "\n",
        "env = CarRacing()\n",
        "\n",
        "for i_episode in range(NUM_OF_EPISODES):\n",
        "\n",
        "    # Initialize the environment and state    \n",
        "    if (i_episode % LOG_VIDEO_EPISODES == 0):\n",
        "        env = gym.wrappers.Monitor(CarRacing(), './video', force=True)\n",
        "    else:\n",
        "        env = CarRacing()\n",
        "        \n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "    for t in range(NUM_OF_STEPS):\n",
        "\n",
        "        ################# Select and perform an action ##################\n",
        "        fw_start = time.time() \n",
        "        action = select_action(state)\n",
        "        fw_end = time.time() \n",
        "        #################################################################\n",
        "\n",
        "\n",
        "        ########################## render step ##########################\n",
        "        render_start = time.time() \n",
        "        _, reward, done, _ = env.step(env.actions[action.item()])\n",
        "        render_end = time.time()\n",
        "        #################################################################\n",
        "\n",
        "\n",
        "        ####################### Observe new state #######################\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "        next_state_time = time.time() \n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "        #################################################################\n",
        "\n",
        "\n",
        "        #################################################################\n",
        "        # Perform one step of the optimization (on the target network) ##\n",
        "        optimize_model()\n",
        "        optimize_time = time.time() \n",
        "        #################################################################\n",
        "\n",
        "        if DEBUG:\n",
        "            print(\"step \", t)\n",
        "            print(\"fw time \",fw_end- fw_start)\n",
        "            print(\"render time \", render_end-render_start)\n",
        "            print(\"next state time \", next_state_time- render_end)\n",
        "            print(\"otpimize_time \", optimize_time- next_state_time)\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            break\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "    env.close() \n",
        "\n",
        "    # render gameplay video and save model checkpoint\n",
        "    if (i_episode % LOG_VIDEO_EPISODES == 0):\n",
        "        \n",
        "        # Save your model and optimizer\n",
        "        torch.save(policy_net.state_dict(), MODEL_SAVE_NAME+\".pth\")\n",
        "        torch.save(optimizer.state_dict(), OPTIMIZER_SAVE_NAME+\".pth\")\n",
        "        # Save as artifact for version control.\n",
        "        artifact = wandb.Artifact(MODEL_SAVE_NAME, type='model')\n",
        "        artifact.add_file(MODEL_SAVE_NAME+\".pth\")\n",
        "        artifact.add_file(OPTIMIZER_SAVE_NAME+\".pth\")\n",
        "        wandb.log_artifact(artifact)\n",
        "        # run.join()\n",
        "\n",
        "\n",
        "        mp4list = glob.glob('video/*.mp4')\n",
        "        print(mp4list)\n",
        "        if len(mp4list) > 0:\n",
        "            print(len(mp4list))\n",
        "            mp4 = mp4list[-1]\n",
        "            video = io.open(mp4, 'r+b').read()\n",
        "            encoded = base64.b64encode(video)\n",
        "\n",
        "            # log gameplay video in wandb\n",
        "            wandb.log({\"gameplays\": wandb.Video(mp4, fps=4, format=\"gif\")})\n",
        "\n",
        "            # display gameplay video\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(HTML(data='''<video alt=\"\" autoplay \n",
        "                        loop controls style=\"height: 400px;\">\n",
        "                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                    </video>'''.format(encoded.decode('ascii'))))\n",
        "            print(\"Episode \",i_episode)\n",
        "\n",
        "print('Complete')\n",
        "# env.render()\n",
        "# env.close()\n",
        "# plt.ioff()\n",
        "# plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"\" autoplay \n",
              "                        loop controls style=\"height: 400px;\">\n",
              "                        <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "                    </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Episode  0\n",
            "Track generation: 1035..1297 -> 262-tiles track\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WlicSGrgRY-"
      },
      "source": [
        "## train new "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APthcuopTOg4"
      },
      "source": [
        "import io\n",
        "import pathlib\n",
        "import time\n",
        "import warnings\n",
        "from typing import Any, Dict, Optional, Tuple, Type, Union, Callable\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch as th\n",
        "\n",
        "from stable_baselines3.common import logger\n",
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "from stable_baselines3.common.buffers import ReplayBuffer\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.noise import ActionNoise\n",
        "from stable_baselines3.common.policies import BasePolicy\n",
        "from stable_baselines3.common.save_util import load_from_pkl, save_to_pkl\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, RolloutReturn\n",
        "Schedule = Callable[[float], float]\n",
        "from stable_baselines3.common.utils import safe_mean\n",
        "from stable_baselines3.common.vec_env import VecEnv\n",
        "\n",
        "\n",
        "class OffPolicyAlgorithm(BaseAlgorithm):\n",
        "    \"\"\"\n",
        "    The base for Off-Policy algorithms (ex: SAC/TD3)\n",
        "    :param policy: Policy object\n",
        "    :param env: The environment to learn from\n",
        "                (if registered in Gym, can be str. Can be None for loading trained models)\n",
        "    :param policy_base: The base policy used by this method\n",
        "    :param learning_rate: learning rate for the optimizer,\n",
        "        it can be a function of the current progress remaining (from 1 to 0)\n",
        "    :param buffer_size: size of the replay buffer\n",
        "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
        "    :param batch_size: Minibatch size for each gradient update\n",
        "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1)\n",
        "    :param gamma: the discount factor\n",
        "    :param train_freq: Update the model every ``train_freq`` steps. Set to `-1` to disable.\n",
        "    :param gradient_steps: How many gradient steps to do after each rollout\n",
        "        (see ``train_freq`` and ``n_episodes_rollout``)\n",
        "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
        "        during the rollout.\n",
        "    :param n_episodes_rollout: Update the model every ``n_episodes_rollout`` episodes.\n",
        "        Note that this cannot be used at the same time as ``train_freq``. Set to `-1` to disable.\n",
        "    :param action_noise: the action noise type (None by default), this can help\n",
        "        for hard exploration problem. Cf common.noise for the different action noise type.\n",
        "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
        "        at a cost of more complexity.\n",
        "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
        "    :param policy_kwargs: Additional arguments to be passed to the policy on creation\n",
        "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
        "    :param verbose: The verbosity level: 0 none, 1 training information, 2 debug\n",
        "    :param device: Device on which the code should run.\n",
        "        By default, it will try to use a Cuda compatible device and fallback to cpu\n",
        "        if it is not possible.\n",
        "    :param support_multi_env: Whether the algorithm supports training\n",
        "        with multiple environments (as in A2C)\n",
        "    :param create_eval_env: Whether to create a second environment that will be\n",
        "        used for evaluating the agent periodically. (Only available when passing string for the environment)\n",
        "    :param monitor_wrapper: When creating an environment, whether to wrap it\n",
        "        or not in a Monitor wrapper.\n",
        "    :param seed: Seed for the pseudo random generators\n",
        "    :param use_sde: Whether to use State Dependent Exploration (SDE)\n",
        "        instead of action noise exploration (default: False)\n",
        "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
        "        Default: -1 (only sample at the beginning of the rollout)\n",
        "    :param use_sde_at_warmup: Whether to use gSDE instead of uniform sampling\n",
        "        during the warm up phase (before learning starts)\n",
        "    :param sde_support: Whether the model support gSDE or not\n",
        "    :param remove_time_limit_termination: Remove terminations (dones) that are due to time limit.\n",
        "        See https://github.com/hill-a/stable-baselines/issues/863\n",
        "    :param supported_action_spaces: The action spaces supported by the algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: Type[BasePolicy],\n",
        "        env: Union[GymEnv, str],\n",
        "        policy_base: Type[BasePolicy],\n",
        "        learning_rate: Union[float, Schedule],\n",
        "        buffer_size: int = int(1e6),\n",
        "        learning_starts: int = 100,\n",
        "        batch_size: int = 256,\n",
        "        tau: float = 0.005,\n",
        "        gamma: float = 0.99,\n",
        "        train_freq: int = 1,\n",
        "        gradient_steps: int = 1,\n",
        "        n_episodes_rollout: int = -1,\n",
        "        action_noise: Optional[ActionNoise] = None,\n",
        "        optimize_memory_usage: bool = False,\n",
        "        policy_kwargs: Dict[str, Any] = None,\n",
        "        tensorboard_log: Optional[str] = None,\n",
        "        verbose: int = 0,\n",
        "        device: Union[th.device, str] = \"auto\",\n",
        "        support_multi_env: bool = False,\n",
        "        create_eval_env: bool = False,\n",
        "        monitor_wrapper: bool = True,\n",
        "        seed: Optional[int] = None,\n",
        "        use_sde: bool = False,\n",
        "        sde_sample_freq: int = -1,\n",
        "        use_sde_at_warmup: bool = False,\n",
        "        sde_support: bool = True,\n",
        "        remove_time_limit_termination: bool = False,\n",
        "        supported_action_spaces: Optional[Tuple[gym.spaces.Space, ...]] = None,\n",
        "    ):\n",
        "\n",
        "        super(OffPolicyAlgorithm, self).__init__(\n",
        "            policy=policy,\n",
        "            env=env,\n",
        "            policy_base=policy_base,\n",
        "            learning_rate=learning_rate,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            verbose=verbose,\n",
        "            device=device,\n",
        "            support_multi_env=support_multi_env,\n",
        "            create_eval_env=create_eval_env,\n",
        "            monitor_wrapper=monitor_wrapper,\n",
        "            seed=seed,\n",
        "            use_sde=use_sde,\n",
        "            sde_sample_freq=sde_sample_freq,\n",
        "            supported_action_spaces=supported_action_spaces,\n",
        "        )\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_starts = learning_starts\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "        self.train_freq = train_freq\n",
        "        self.gradient_steps = gradient_steps\n",
        "        self.n_episodes_rollout = n_episodes_rollout\n",
        "        self.action_noise = action_noise\n",
        "        self.optimize_memory_usage = optimize_memory_usage\n",
        "\n",
        "        # Remove terminations (dones) that are due to time limit\n",
        "        # see https://github.com/hill-a/stable-baselines/issues/863\n",
        "        self.remove_time_limit_termination = remove_time_limit_termination\n",
        "\n",
        "        if train_freq > 0 and n_episodes_rollout > 0:\n",
        "            warnings.warn(\n",
        "                \"You passed a positive value for `train_freq` and `n_episodes_rollout`.\"\n",
        "                \"Please make sure this is intended. \"\n",
        "                \"The agent will collect data by stepping in the environment \"\n",
        "                \"until both conditions are true: \"\n",
        "                \"`number of steps in the env` >= `train_freq` and \"\n",
        "                \"`number of episodes` > `n_episodes_rollout`\"\n",
        "            )\n",
        "\n",
        "        self.actor = None  # type: Optional[th.nn.Module]\n",
        "        self.replay_buffer = None  # type: Optional[ReplayBuffer]\n",
        "        # Update policy keyword arguments\n",
        "        if sde_support:\n",
        "            self.policy_kwargs[\"use_sde\"] = self.use_sde\n",
        "        # For gSDE only\n",
        "        self.use_sde_at_warmup = use_sde_at_warmup\n",
        "\n",
        "    def _setup_model(self) -> None:\n",
        "        self._setup_lr_schedule()\n",
        "        self.set_random_seed(self.seed)\n",
        "        self.replay_buffer = ReplayBuffer(\n",
        "            self.buffer_size,\n",
        "            self.observation_space,\n",
        "            self.action_space,\n",
        "            self.device,\n",
        "            optimize_memory_usage=self.optimize_memory_usage,\n",
        "        )\n",
        "        self.policy = self.policy_class(\n",
        "            self.observation_space,\n",
        "            self.action_space,\n",
        "            self.lr_schedule,\n",
        "            **self.policy_kwargs  # pytype:disable=not-instantiable\n",
        "        )\n",
        "        self.policy = self.policy.to(self.device)\n",
        "\n",
        "    def save_replay_buffer(self, path: Union[str, pathlib.Path, io.BufferedIOBase]) -> None:\n",
        "        \"\"\"\n",
        "        Save the replay buffer as a pickle file.\n",
        "        :param path: Path to the file where the replay buffer should be saved.\n",
        "            if path is a str or pathlib.Path, the path is automatically created if necessary.\n",
        "        \"\"\"\n",
        "        assert self.replay_buffer is not None, \"The replay buffer is not defined\"\n",
        "        save_to_pkl(path, self.replay_buffer, self.verbose)\n",
        "\n",
        "    def load_replay_buffer(self, path: Union[str, pathlib.Path, io.BufferedIOBase]) -> None:\n",
        "        \"\"\"\n",
        "        Load a replay buffer from a pickle file.\n",
        "        :param path: Path to the pickled replay buffer.\n",
        "        \"\"\"\n",
        "        self.replay_buffer = load_from_pkl(path, self.verbose)\n",
        "        assert isinstance(self.replay_buffer, ReplayBuffer), \"The replay buffer must inherit from ReplayBuffer class\"\n",
        "\n",
        "    def _setup_learn(\n",
        "        self,\n",
        "        total_timesteps: int,\n",
        "        eval_env: Optional[GymEnv],\n",
        "        callback: MaybeCallback = None,\n",
        "        eval_freq: int = 10000,\n",
        "        n_eval_episodes: int = 5,\n",
        "        log_path: Optional[str] = None,\n",
        "        reset_num_timesteps: bool = True,\n",
        "        tb_log_name: str = \"run\",\n",
        "    ) -> Tuple[int, BaseCallback]:\n",
        "        \"\"\"\n",
        "        cf `BaseAlgorithm`.\n",
        "        \"\"\"\n",
        "        # Prevent continuity issue by truncating trajectory\n",
        "        # when using memory efficient replay buffer\n",
        "        # see https://github.com/DLR-RM/stable-baselines3/issues/46\n",
        "        truncate_last_traj = (\n",
        "            self.optimize_memory_usage\n",
        "            and reset_num_timesteps\n",
        "            and self.replay_buffer is not None\n",
        "            and (self.replay_buffer.full or self.replay_buffer.pos > 0)\n",
        "        )\n",
        "\n",
        "        if truncate_last_traj:\n",
        "            warnings.warn(\n",
        "                \"The last trajectory in the replay buffer will be truncated, \"\n",
        "                \"see https://github.com/DLR-RM/stable-baselines3/issues/46.\"\n",
        "                \"You should use `reset_num_timesteps=False` or `optimize_memory_usage=False`\"\n",
        "                \"to avoid that issue.\"\n",
        "            )\n",
        "            # Go to the previous index\n",
        "            pos = (self.replay_buffer.pos - 1) % self.replay_buffer.buffer_size\n",
        "            self.replay_buffer.dones[pos] = True\n",
        "\n",
        "        return super()._setup_learn(\n",
        "            total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name\n",
        "        )\n",
        "\n",
        "    def learn(\n",
        "        self,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 4,\n",
        "        eval_env: Optional[GymEnv] = None,\n",
        "        eval_freq: int = -1,\n",
        "        n_eval_episodes: int = 5,\n",
        "        tb_log_name: str = \"run\",\n",
        "        eval_log_path: Optional[str] = None,\n",
        "        reset_num_timesteps: bool = True,\n",
        "    ) -> \"OffPolicyAlgorithm\":\n",
        "\n",
        "        total_timesteps, callback = self._setup_learn(\n",
        "            total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, eval_log_path, reset_num_timesteps, tb_log_name\n",
        "        )\n",
        "\n",
        "        callback.on_training_start(locals(), globals())\n",
        "\n",
        "        while self.num_timesteps < total_timesteps:\n",
        "\n",
        "            rollout = self.collect_rollouts(\n",
        "                self.env,\n",
        "                n_episodes=self.n_episodes_rollout,\n",
        "                n_steps=self.train_freq,\n",
        "                action_noise=self.action_noise,\n",
        "                callback=callback,\n",
        "                learning_starts=self.learning_starts,\n",
        "                replay_buffer=self.replay_buffer,\n",
        "                log_interval=log_interval,\n",
        "            )\n",
        "\n",
        "            if rollout.continue_training is False:\n",
        "                break\n",
        "\n",
        "            if self.num_timesteps > 0 and self.num_timesteps > self.learning_starts:\n",
        "                # If no `gradient_steps` is specified,\n",
        "                # do as many gradients steps as steps performed during the rollout\n",
        "                gradient_steps = self.gradient_steps if self.gradient_steps > 0 else rollout.episode_timesteps\n",
        "                self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)\n",
        "\n",
        "        callback.on_training_end()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def train(self, gradient_steps: int, batch_size: int) -> None:\n",
        "        \"\"\"\n",
        "        Sample the replay buffer and do the updates\n",
        "        (gradient descent and update target networks)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _sample_action(\n",
        "        self, learning_starts: int, action_noise: Optional[ActionNoise] = None\n",
        "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Sample an action according to the exploration policy.\n",
        "        This is either done by sampling the probability distribution of the policy,\n",
        "        or sampling a random action (from a uniform distribution over the action space)\n",
        "        or by adding noise to the deterministic output.\n",
        "        :param action_noise: Action noise that will be used for exploration\n",
        "            Required for deterministic policy (e.g. TD3). This can also be used\n",
        "            in addition to the stochastic policy for SAC.\n",
        "        :param learning_starts: Number of steps before learning for the warm-up phase.\n",
        "        :return: action to take in the environment\n",
        "            and scaled action that will be stored in the replay buffer.\n",
        "            The two differs when the action space is not normalized (bounds are not [-1, 1]).\n",
        "        \"\"\"\n",
        "        # Select action randomly or according to policy\n",
        "        if self.num_timesteps < learning_starts and not (self.use_sde and self.use_sde_at_warmup):\n",
        "            # Warmup phase\n",
        "            unscaled_action = np.array([self.action_space.sample()])\n",
        "        else:\n",
        "            # Note: when using continuous actions,\n",
        "            # we assume that the policy uses tanh to scale the action\n",
        "            # We use non-deterministic action in the case of SAC, for TD3, it does not matter\n",
        "            unscaled_action, _ = self.predict(self._last_obs, deterministic=False)\n",
        "\n",
        "        # Rescale the action from [low, high] to [-1, 1]\n",
        "        if isinstance(self.action_space, gym.spaces.Box):\n",
        "            scaled_action = self.policy.scale_action(unscaled_action)\n",
        "\n",
        "            # Add noise to the action (improve exploration)\n",
        "            if action_noise is not None:\n",
        "                scaled_action = np.clip(scaled_action + action_noise(), -1, 1)\n",
        "\n",
        "            # We store the scaled action in the buffer\n",
        "            buffer_action = scaled_action\n",
        "            action = self.policy.unscale_action(scaled_action)\n",
        "        else:\n",
        "            # Discrete case, no need to normalize or clip\n",
        "            buffer_action = unscaled_action\n",
        "            action = buffer_action\n",
        "        return action, buffer_action\n",
        "\n",
        "    def _dump_logs(self) -> None:\n",
        "        \"\"\"\n",
        "        Write log.\n",
        "        \"\"\"\n",
        "        fps = int(self.num_timesteps / (time.time() - self.start_time))\n",
        "        logger.record(\"time/episodes\", self._episode_num, exclude=\"tensorboard\")\n",
        "        if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n",
        "            logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]))\n",
        "            logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]))\n",
        "        logger.record(\"time/fps\", fps)\n",
        "        logger.record(\"time/time_elapsed\", int(time.time() - self.start_time), exclude=\"tensorboard\")\n",
        "        logger.record(\"time/total timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
        "        if self.use_sde:\n",
        "            logger.record(\"train/std\", (self.actor.get_std()).mean().item())\n",
        "\n",
        "        if len(self.ep_success_buffer) > 0:\n",
        "            logger.record(\"rollout/success rate\", safe_mean(self.ep_success_buffer))\n",
        "        # Pass the number of timesteps for tensorboard\n",
        "        logger.dump(step=self.num_timesteps)\n",
        "\n",
        "    def _on_step(self) -> None:\n",
        "        \"\"\"\n",
        "        Method called after each step in the environment.\n",
        "        It is meant to trigger DQN target network update\n",
        "        but can be used for other purposes\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def collect_rollouts(\n",
        "        self,\n",
        "        env: VecEnv,\n",
        "        callback: BaseCallback,\n",
        "        n_episodes: int = 1,\n",
        "        n_steps: int = -1,\n",
        "        action_noise: Optional[ActionNoise] = None,\n",
        "        learning_starts: int = 0,\n",
        "        replay_buffer: Optional[ReplayBuffer] = None,\n",
        "        log_interval: Optional[int] = None,\n",
        "    ) -> RolloutReturn:\n",
        "        \"\"\"\n",
        "        Collect experiences and store them into a ``ReplayBuffer``.\n",
        "        :param env: The training environment\n",
        "        :param callback: Callback that will be called at each step\n",
        "            (and at the beginning and end of the rollout)\n",
        "        :param n_episodes: Number of episodes to use to collect rollout data\n",
        "            You can also specify a ``n_steps`` instead\n",
        "        :param n_steps: Number of steps to use to collect rollout data\n",
        "            You can also specify a ``n_episodes`` instead.\n",
        "        :param action_noise: Action noise that will be used for exploration\n",
        "            Required for deterministic policy (e.g. TD3). This can also be used\n",
        "            in addition to the stochastic policy for SAC.\n",
        "        :param learning_starts: Number of steps before learning for the warm-up phase.\n",
        "        :param replay_buffer:\n",
        "        :param log_interval: Log data every ``log_interval`` episodes\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        episode_rewards, total_timesteps = [], []\n",
        "        total_steps, total_episodes = 0, 0\n",
        "\n",
        "        assert isinstance(env, VecEnv), \"You must pass a VecEnv\"\n",
        "        assert env.num_envs == 1, \"OffPolicyAlgorithm only support single environment\"\n",
        "\n",
        "        if self.use_sde:\n",
        "            self.actor.reset_noise()\n",
        "\n",
        "        callback.on_rollout_start()\n",
        "        continue_training = True\n",
        "\n",
        "        while total_steps < n_steps or total_episodes < n_episodes:\n",
        "            done = False\n",
        "            episode_reward, episode_timesteps = 0.0, 0\n",
        "\n",
        "            while not done:\n",
        "\n",
        "                if self.use_sde and self.sde_sample_freq > 0 and total_steps % self.sde_sample_freq == 0:\n",
        "                    # Sample a new noise matrix\n",
        "                    self.actor.reset_noise()\n",
        "\n",
        "                # Select action randomly or according to policy\n",
        "                action, buffer_action = self._sample_action(learning_starts, action_noise)\n",
        "\n",
        "                # Rescale and perform action\n",
        "                new_obs, reward, done, infos = env.step(action)\n",
        "\n",
        "                self.num_timesteps += 1\n",
        "                episode_timesteps += 1\n",
        "                total_steps += 1\n",
        "\n",
        "                # Give access to local variables\n",
        "                callback.update_locals(locals())\n",
        "                # Only stop training if return value is False, not when it is None.\n",
        "                if callback.on_step() is False:\n",
        "                    return RolloutReturn(0.0, total_steps, total_episodes, continue_training=False)\n",
        "\n",
        "                episode_reward += reward\n",
        "\n",
        "                # Retrieve reward and episode length if using Monitor wrapper\n",
        "                self._update_info_buffer(infos, done)\n",
        "\n",
        "                # Store data in replay buffer\n",
        "                if replay_buffer is not None:\n",
        "                    # Store only the unnormalized version\n",
        "                    if self._vec_normalize_env is not None:\n",
        "                        new_obs_ = self._vec_normalize_env.get_original_obs()\n",
        "                        reward_ = self._vec_normalize_env.get_original_reward()\n",
        "                    else:\n",
        "                        # Avoid changing the original ones\n",
        "                        self._last_original_obs, new_obs_, reward_ = self._last_obs, new_obs, reward\n",
        "\n",
        "                    replay_buffer.add(self._last_original_obs, new_obs_, buffer_action, reward_, done)\n",
        "\n",
        "                self._last_obs = new_obs\n",
        "                # Save the unnormalized observation\n",
        "                if self._vec_normalize_env is not None:\n",
        "                    self._last_original_obs = new_obs_\n",
        "\n",
        "                self._update_current_progress_remaining(self.num_timesteps, self._total_timesteps)\n",
        "\n",
        "                # For DQN, check if the target network should be updated\n",
        "                # and update the exploration schedule\n",
        "                # For SAC/TD3, the update is done as the same time as the gradient update\n",
        "                # see https://github.com/hill-a/stable-baselines/issues/900\n",
        "                self._on_step()\n",
        "\n",
        "                if 0 < n_steps <= total_steps:\n",
        "                    break\n",
        "\n",
        "            if done:\n",
        "                total_episodes += 1\n",
        "                self._episode_num += 1\n",
        "                episode_rewards.append(episode_reward)\n",
        "                total_timesteps.append(episode_timesteps)\n",
        "\n",
        "                if action_noise is not None:\n",
        "                    action_noise.reset()\n",
        "\n",
        "                # Log training infos\n",
        "                if log_interval is not None and self._episode_num % log_interval == 0:\n",
        "                    self._dump_logs()\n",
        "\n",
        "        mean_reward = np.mean(episode_rewards) if total_episodes > 0 else 0.0\n",
        "\n",
        "        callback.on_rollout_end()\n",
        "\n",
        "        return RolloutReturn(mean_reward, total_steps, total_episodes, continue_training)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs_pO5jiL0GG"
      },
      "source": [
        "from typing import Any, Dict, List, Optional, Tuple, Type, Union, Callable\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from stable_baselines3.common import logger\n",
        "# from stable_baselines3.common.off_policy_algorithm import OffPolicyAlgorithm\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback\n",
        "Schedule = Callable[[float], float]\n",
        "from stable_baselines3.common.utils import get_linear_fn, is_vectorized_observation, polyak_update\n",
        "from stable_baselines3.dqn.policies import DQNPolicy\n",
        "\n",
        "\n",
        "class DQN(OffPolicyAlgorithm):\n",
        "    \"\"\"\n",
        "    Deep Q-Network (DQN)\n",
        "    Paper: https://arxiv.org/abs/1312.5602, https://www.nature.com/articles/nature14236\n",
        "    Default hyperparameters are taken from the nature paper,\n",
        "    except for the optimizer and learning rate that were taken from Stable Baselines defaults.\n",
        "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
        "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
        "    :param learning_rate: The learning rate, it can be a function\n",
        "        of the current progress remaining (from 1 to 0)\n",
        "    :param buffer_size: size of the replay buffer\n",
        "    :param learning_starts: how many steps of the model to collect transitions for before learning starts\n",
        "    :param batch_size: Minibatch size for each gradient update\n",
        "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
        "    :param gamma: the discount factor\n",
        "    :param train_freq: Update the model every ``train_freq`` steps. Set to `-1` to disable.\n",
        "    :param gradient_steps: How many gradient steps to do after each rollout\n",
        "        (see ``train_freq`` and ``n_episodes_rollout``)\n",
        "        Set to ``-1`` means to do as many gradient steps as steps done in the environment\n",
        "        during the rollout.\n",
        "    :param n_episodes_rollout: Update the model every ``n_episodes_rollout`` episodes.\n",
        "        Note that this cannot be used at the same time as ``train_freq``. Set to `-1` to disable.\n",
        "    :param optimize_memory_usage: Enable a memory efficient variant of the replay buffer\n",
        "        at a cost of more complexity.\n",
        "        See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
        "    :param target_update_interval: update the target network every ``target_update_interval``\n",
        "        environment steps.\n",
        "    :param exploration_fraction: fraction of entire training period over which the exploration rate is reduced\n",
        "    :param exploration_initial_eps: initial value of random action probability\n",
        "    :param exploration_final_eps: final value of random action probability\n",
        "    :param max_grad_norm: The maximum value for the gradient clipping\n",
        "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
        "    :param create_eval_env: Whether to create a second environment that will be\n",
        "        used for evaluating the agent periodically. (Only available when passing string for the environment)\n",
        "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
        "    :param verbose: the verbosity level: 0 no output, 1 info, 2 debug\n",
        "    :param seed: Seed for the pseudo random generators\n",
        "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
        "        Setting it to auto, the code will be run on the GPU if possible.\n",
        "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: Union[str, Type[DQNPolicy]],\n",
        "        env: Union[GymEnv, str],\n",
        "        learning_rate: Union[float, Schedule] = 1e-4,\n",
        "        buffer_size: int = 1000000,\n",
        "        learning_starts: int = 50000,\n",
        "        batch_size: Optional[int] = 32,\n",
        "        tau: float = 1.0,\n",
        "        gamma: float = 0.99,\n",
        "        train_freq: int = 4,\n",
        "        gradient_steps: int = 1,\n",
        "        n_episodes_rollout: int = -1,\n",
        "        optimize_memory_usage: bool = False,\n",
        "        target_update_interval: int = 10000,\n",
        "        exploration_fraction: float = 0.1,\n",
        "        exploration_initial_eps: float = 1.0,\n",
        "        exploration_final_eps: float = 0.05,\n",
        "        max_grad_norm: float = 10,\n",
        "        tensorboard_log: Optional[str] = None,\n",
        "        create_eval_env: bool = False,\n",
        "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        verbose: int = 0,\n",
        "        seed: Optional[int] = None,\n",
        "        device: Union[th.device, str] = \"auto\",\n",
        "        _init_setup_model: bool = True,\n",
        "    ):\n",
        "        print(\"naiii\")\n",
        "        super(DQN, self).__init__(\n",
        "            policy,\n",
        "            env,\n",
        "            DQNPolicy,\n",
        "            learning_rate,\n",
        "            buffer_size,\n",
        "            learning_starts,\n",
        "            batch_size,\n",
        "            tau,\n",
        "            gamma,\n",
        "            train_freq,\n",
        "            gradient_steps,\n",
        "            n_episodes_rollout,\n",
        "            action_noise=None,  # No action noise\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            verbose=verbose,\n",
        "            device=device,\n",
        "            create_eval_env=create_eval_env,\n",
        "            seed=seed,\n",
        "            sde_support=False,\n",
        "            optimize_memory_usage=optimize_memory_usage,\n",
        "            supported_action_spaces=(gym.spaces.Discrete,),\n",
        "        )\n",
        "\n",
        "        self.exploration_initial_eps = exploration_initial_eps\n",
        "        self.exploration_final_eps = exploration_final_eps\n",
        "        self.exploration_fraction = exploration_fraction\n",
        "        self.target_update_interval = target_update_interval\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        # \"epsilon\" for the epsilon-greedy exploration\n",
        "        self.exploration_rate = 0.0\n",
        "        # Linear schedule will be defined in `_setup_model()`\n",
        "        self.exploration_schedule = None\n",
        "        self.q_net, self.q_net_target = None, None\n",
        "\n",
        "        if _init_setup_model:\n",
        "            self._setup_model()\n",
        "\n",
        "    def _setup_model(self) -> None:\n",
        "        super(DQN, self)._setup_model()\n",
        "        self._create_aliases()\n",
        "        self.exploration_schedule = get_linear_fn(\n",
        "            self.exploration_initial_eps, self.exploration_final_eps, self.exploration_fraction\n",
        "        )\n",
        "\n",
        "    def _create_aliases(self) -> None:\n",
        "        self.q_net = self.policy.q_net\n",
        "        self.q_net_target = self.policy.q_net_target\n",
        "\n",
        "    def _on_step(self) -> None:\n",
        "        \"\"\"\n",
        "        Update the exploration rate and target network if needed.\n",
        "        This method is called in ``collect_rollouts()`` after each step in the environment.\n",
        "        \"\"\"\n",
        "        if self.num_timesteps % self.target_update_interval == 0:\n",
        "            polyak_update(self.q_net.parameters(), self.q_net_target.parameters(), self.tau)\n",
        "\n",
        "        self.exploration_rate = self.exploration_schedule(self._current_progress_remaining)\n",
        "        logger.record(\"rollout/exploration rate\", self.exploration_rate)\n",
        "\n",
        "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
        "        # Update learning rate according to schedule\n",
        "        self._update_learning_rate(self.policy.optimizer)\n",
        "\n",
        "        losses = []\n",
        "        for gradient_step in range(gradient_steps):\n",
        "            # Sample replay buffer\n",
        "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n",
        "\n",
        "            with th.no_grad():\n",
        "                # Compute the next Q-values using the target network\n",
        "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
        "                # Follow greedy policy: use the one with the highest value\n",
        "                next_q_values, _ = next_q_values.max(dim=1)\n",
        "                # Avoid potential broadcast issue\n",
        "                next_q_values = next_q_values.reshape(-1, 1)\n",
        "                # 1-step TD target\n",
        "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
        "\n",
        "            # Get current Q-values estimates\n",
        "            current_q_values = self.q_net(replay_data.observations)\n",
        "\n",
        "            # Retrieve the q-values for the actions from the replay buffer\n",
        "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
        "\n",
        "            # Compute Huber loss (less sensitive to outliers)\n",
        "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Optimize the policy\n",
        "            self.policy.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # Clip gradient norm\n",
        "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "            self.policy.optimizer.step()\n",
        "\n",
        "        # Increase update counter\n",
        "        self._n_updates += gradient_steps\n",
        "\n",
        "        logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "        logger.record(\"train/loss\", np.mean(losses))\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        observation: np.ndarray,\n",
        "        state: Optional[np.ndarray] = None,\n",
        "        mask: Optional[np.ndarray] = None,\n",
        "        deterministic: bool = False,\n",
        "    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Overrides the base_class predict function to include epsilon-greedy exploration.\n",
        "        :param observation: the input observation\n",
        "        :param state: The last states (can be None, used in recurrent policies)\n",
        "        :param mask: The last masks (can be None, used in recurrent policies)\n",
        "        :param deterministic: Whether or not to return deterministic actions.\n",
        "        :return: the model's action and the next state\n",
        "            (used in recurrent policies)\n",
        "        \"\"\"\n",
        "        if not deterministic and np.random.rand() < self.exploration_rate:\n",
        "            if is_vectorized_observation(observation, self.observation_space):\n",
        "                n_batch = observation.shape[0]\n",
        "                action = np.array([self.action_space.sample() for _ in range(n_batch)])\n",
        "            else:\n",
        "                action = np.array(self.action_space.sample())\n",
        "        else:\n",
        "            action, state = self.policy.predict(observation, state, mask, deterministic)\n",
        "        return action, state\n",
        "\n",
        "    def learn(\n",
        "        self,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 4,\n",
        "        eval_env: Optional[GymEnv] = None,\n",
        "        eval_freq: int = -1,\n",
        "        n_eval_episodes: int = 5,\n",
        "        tb_log_name: str = \"DQN\",\n",
        "        eval_log_path: Optional[str] = None,\n",
        "        reset_num_timesteps: bool = True,\n",
        "    ) -> OffPolicyAlgorithm:\n",
        "\n",
        "        return super(DQN, self).learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=callback,\n",
        "            log_interval=log_interval,\n",
        "            eval_env=eval_env,\n",
        "            eval_freq=eval_freq,\n",
        "            n_eval_episodes=n_eval_episodes,\n",
        "            tb_log_name=tb_log_name,\n",
        "            eval_log_path=eval_log_path,\n",
        "            reset_num_timesteps=reset_num_timesteps,\n",
        "        )\n",
        "\n",
        "    def _excluded_save_params(self) -> List[str]:\n",
        "        return super(DQN, self)._excluded_save_params() + [\"q_net\", \"q_net_target\"]\n",
        "\n",
        "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
        "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
        "\n",
        "        return state_dicts, []"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN2SUkuJn7W1"
      },
      "source": [
        "## ÎµÏ€Î¹Ï„ÎµÎ»Î¿Î¸Ïƒ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4I2u651MNXAB",
        "outputId": "303e4ea2-b952-4099-894b-7044c3a70b79"
      },
      "source": [
        "np.__version__"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.15.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5ayf9GSMSUs"
      },
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "\n",
        "class CustomCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    A custom callback that derives from ``BaseCallback``.\n",
        "\n",
        "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose=0):\n",
        "        super(CustomCallback, self).__init__(verbose)\n",
        "        # Those variables will be accessible in the callback\n",
        "        # (they are defined in the base class)\n",
        "        # The RL model\n",
        "        # self.model = None  # type: BaseAlgorithm\n",
        "        # An alias for self.model.get_env(), the environment used for training\n",
        "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
        "        # Number of time the callback was called\n",
        "        # self.n_calls = 0  # type: int\n",
        "        # self.num_timesteps = 0  # type: int\n",
        "        # local and global variables\n",
        "        # self.locals = None  # type: Dict[str, Any]\n",
        "        # self.globals = None  # type: Dict[str, Any]\n",
        "        # The logger object, used to report things in the terminal\n",
        "        # self.logger = None  # stable_baselines3.common.logger\n",
        "        # # Sometimes, for event callback, it is useful\n",
        "        # # to have access to the parent object\n",
        "        # self.parent = None  # type: Optional[BaseCallback]\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_rollout_start(self) -> None:\n",
        "        \"\"\"\n",
        "        A rollout is the collection of environment interaction\n",
        "        using the current policy.\n",
        "        This event is triggered before collecting new samples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # print(\"pipaa\", self.num_timesteps)\n",
        "        # print(self.locals[\"done\"])\n",
        "        if self.locals[\"done\"][0]:\n",
        "            print(self.locals)\n",
        "            print(\"PIPAAAAAAAAAAAAAAAAA\", self.locals[\"total_episodes\"])\n",
        "        \n",
        "        return True\n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before updating the policy.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVcpDbXiEPD7",
        "outputId": "346833cf-b729-4726-90b0-7f138bb319fc"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.dqn import CnnPolicy\n",
        "\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EveryNTimesteps\n",
        "\n",
        "checkpoint_on_event = CheckpointCallback(save_freq=1, save_path='./logs/')\n",
        "event_callback = EveryNTimesteps(n_steps=500, callback=checkpoint_on_event)\n",
        "\n",
        "\n",
        "# env = gym.wrappers.Monitor(CarRacing(), './video', force=True)\n",
        "# env = gnwrapper.Animation(CarRacing())\n",
        "\n",
        "env = CarRacing()\n",
        "\n",
        "\n",
        "model = DQN(CnnPolicy, CarRacing(100), verbose=1, buffer_size=1000, learning_starts=1000)\n",
        "model.learn(total_timesteps=500, log_interval=4, callback=CustomCallback())\n",
        "model.save(\"dqn_pendulum\")\n",
        "\n",
        "# del model # remove to demonstrate saving and loading\n",
        "\n",
        "# model = DQN.load(\"dqn_pendulum\")\n",
        "\n",
        "# obs = env.reset()\n",
        "# while True:\n",
        "#     action, _states = model.predict(obs, deterministic=True)\n",
        "#     obs, reward, done, info = env.step(action)\n",
        "#     env.render()\n",
        "#     if done:\n",
        "#       obs = env.reset()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "Track generation: 1164..1469 -> 305-tiles track\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:427: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Track generation: 1074..1347 -> 273-tiles track\n",
            "{'reset_num_timesteps': True, 'eval_log_path': None, 'tb_log_name': 'DQN', 'n_eval_episodes': 5, 'eval_freq': -1, 'eval_env': None, 'log_interval': 4, 'callback': <__main__.CustomCallback object at 0x7fc875d6a320>, 'total_timesteps': [], 'self': <stable_baselines3.dqn.dqn.DQN object at 0x7fc875d6a940>, 'infos': [{'terminal_observation': array([[[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       [[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       [[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]],\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]],\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]]], dtype=uint8)}], 'reward': array([-0.1], dtype=float32), 'new_obs': array([[[[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]]]], dtype=uint8), 'buffer_action': array([4]), 'action': array([4]), 'episode_timesteps': 4, 'episode_reward': array([-0.3], dtype=float32), 'done': array([ True]), 'continue_training': True, 'total_episodes': 0, 'total_steps': 4, 'episode_rewards': [], 'replay_buffer': <stable_baselines3.common.buffers.ReplayBuffer object at 0x7fc8746139b0>, 'learning_starts': 1000, 'action_noise': None, 'n_steps': 4, 'n_episodes': -1, 'env': <stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7fc8746470f0>, 'reward_': array([-0.1], dtype=float32), 'new_obs_': array([[[[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]]]], dtype=uint8)}\n",
            "PIPAAAAAAAAAAAAAAAAA 0\n",
            "Track generation: 978..1233 -> 255-tiles track\n",
            "{'reset_num_timesteps': True, 'eval_log_path': None, 'tb_log_name': 'DQN', 'n_eval_episodes': 5, 'eval_freq': -1, 'eval_env': None, 'log_interval': 4, 'callback': <__main__.CustomCallback object at 0x7fc875d6a320>, 'total_timesteps': [], 'self': <stable_baselines3.dqn.dqn.DQN object at 0x7fc875d6a940>, 'infos': [{'terminal_observation': array([[[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       [[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       [[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]],\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]],\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]]], dtype=uint8)}], 'reward': array([-0.1], dtype=float32), 'new_obs': array([[[[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[230, 230, 230, ..., 204, 204, 204],\n",
            "         [230, 230, 230, ..., 204, 204, 204],\n",
            "         [230, 230, 230, ..., 204, 204, 204],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]]]], dtype=uint8), 'buffer_action': array([1]), 'action': array([1]), 'episode_timesteps': 4, 'episode_reward': array([-0.3], dtype=float32), 'done': array([ True]), 'continue_training': True, 'total_episodes': 0, 'total_steps': 4, 'episode_rewards': [], 'replay_buffer': <stable_baselines3.common.buffers.ReplayBuffer object at 0x7fc8746139b0>, 'learning_starts': 1000, 'action_noise': None, 'n_steps': 4, 'n_episodes': -1, 'env': <stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7fc8746470f0>, 'reward_': array([-0.1], dtype=float32), 'new_obs_': array([[[[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]]]], dtype=uint8)}\n",
            "PIPAAAAAAAAAAAAAAAAA 0\n",
            "Track generation: 1201..1513 -> 312-tiles track\n",
            "{'reset_num_timesteps': True, 'eval_log_path': None, 'tb_log_name': 'DQN', 'n_eval_episodes': 5, 'eval_freq': -1, 'eval_env': None, 'log_interval': 4, 'callback': <__main__.CustomCallback object at 0x7fc875d6a320>, 'total_timesteps': [], 'self': <stable_baselines3.dqn.dqn.DQN object at 0x7fc875d6a940>, 'infos': [{'terminal_observation': array([[[102, 230, 102],\n",
            "        [102, 230, 102],\n",
            "        [102, 230, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       [[102, 230, 102],\n",
            "        [102, 230, 102],\n",
            "        [102, 230, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       [[102, 230, 102],\n",
            "        [102, 230, 102],\n",
            "        [102, 230, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]],\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]],\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]]], dtype=uint8)}], 'reward': array([-0.1], dtype=float32), 'new_obs': array([[[[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]]]], dtype=uint8), 'buffer_action': array([4]), 'action': array([4]), 'episode_timesteps': 4, 'episode_reward': array([-0.3], dtype=float32), 'done': array([ True]), 'continue_training': True, 'total_episodes': 0, 'total_steps': 4, 'episode_rewards': [], 'replay_buffer': <stable_baselines3.common.buffers.ReplayBuffer object at 0x7fc8746139b0>, 'learning_starts': 1000, 'action_noise': None, 'n_steps': 4, 'n_episodes': -1, 'env': <stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7fc8746470f0>, 'reward_': array([-0.1], dtype=float32), 'new_obs_': array([[[[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[230, 230, 230, ..., 204, 204, 204],\n",
            "         [230, 230, 230, ..., 204, 204, 204],\n",
            "         [230, 230, 230, ..., 204, 204, 204],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]]]], dtype=uint8)}\n",
            "PIPAAAAAAAAAAAAAAAAA 0\n",
            "Track generation: 1081..1359 -> 278-tiles track\n",
            "retry to generate track (normal if there are not many of this messages)\n",
            "Track generation: 1096..1380 -> 284-tiles track\n",
            "{'reset_num_timesteps': True, 'eval_log_path': None, 'tb_log_name': 'DQN', 'n_eval_episodes': 5, 'eval_freq': -1, 'eval_env': None, 'log_interval': 4, 'callback': <__main__.CustomCallback object at 0x7fc875d6a320>, 'total_timesteps': [], 'self': <stable_baselines3.dqn.dqn.DQN object at 0x7fc875d6a940>, 'infos': [{'terminal_observation': array([[[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       [[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       [[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]],\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]],\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]]], dtype=uint8)}], 'reward': array([-0.1], dtype=float32), 'new_obs': array([[[[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]]]], dtype=uint8), 'buffer_action': array([4]), 'action': array([4]), 'episode_timesteps': 4, 'episode_reward': array([-0.3], dtype=float32), 'done': array([ True]), 'continue_training': True, 'total_episodes': 0, 'total_steps': 4, 'episode_rewards': [], 'replay_buffer': <stable_baselines3.common.buffers.ReplayBuffer object at 0x7fc8746139b0>, 'learning_starts': 1000, 'action_noise': None, 'n_steps': 4, 'n_episodes': -1, 'env': <stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7fc8746470f0>, 'reward_': array([-0.1], dtype=float32), 'new_obs_': array([[[[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]]]], dtype=uint8)}\n",
            "PIPAAAAAAAAAAAAAAAAA 0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 82       |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total timesteps  | 400      |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.341    |\n",
            "|    n_updates        | 250      |\n",
            "----------------------------------\n",
            "Track generation: 1118..1408 -> 290-tiles track\n",
            "{'reset_num_timesteps': True, 'eval_log_path': None, 'tb_log_name': 'DQN', 'n_eval_episodes': 5, 'eval_freq': -1, 'eval_env': None, 'log_interval': 4, 'callback': <__main__.CustomCallback object at 0x7fc875d6a320>, 'total_timesteps': [], 'self': <stable_baselines3.dqn.dqn.DQN object at 0x7fc875d6a940>, 'infos': [{'terminal_observation': array([[[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       [[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       [[102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        ...,\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102],\n",
            "        [102, 204, 102]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]],\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]],\n",
            "\n",
            "       [[  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        ...,\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0],\n",
            "        [  0,   0,   0]]], dtype=uint8)}], 'reward': array([-0.1], dtype=float32), 'new_obs': array([[[[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]]]], dtype=uint8), 'buffer_action': array([1]), 'action': array([1]), 'episode_timesteps': 4, 'episode_reward': array([-0.3], dtype=float32), 'done': array([ True]), 'continue_training': True, 'total_episodes': 0, 'total_steps': 4, 'episode_rewards': [], 'replay_buffer': <stable_baselines3.common.buffers.ReplayBuffer object at 0x7fc8746139b0>, 'learning_starts': 1000, 'action_noise': None, 'n_steps': 4, 'n_episodes': -1, 'env': <stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x7fc8746470f0>, 'reward_': array([-0.1], dtype=float32), 'new_obs_': array([[[[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         [204, 204, 204, ..., 204, 204, 204],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]],\n",
            "\n",
            "        [[102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         [102, 102, 102, ..., 102, 102, 102],\n",
            "         ...,\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0],\n",
            "         [  0,   0,   0, ...,   0,   0,   0]]]], dtype=uint8)}\n",
            "PIPAAAAAAAAAAAAAAAAA 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opwVv4GLw9yS",
        "outputId": "99ad17e5-f9de-49c0-b2b7-f30aec62b0c3"
      },
      "source": [
        "env.observation_space"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (96, 96, 3), uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "BAZBg0ZasyOS",
        "outputId": "70c260ab-cd99-4a96-ca2a-b8e66285c288"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.dqn import MlpPolicy\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "model = DQN(MlpPolicy, env, verbose=1)\n",
        "model.learn(total_timesteps=1, log_interval=4)\n",
        "model.save(\"dqn_pendulum\")\n",
        "\n",
        "del model # remove to demonstrate saving and loading\n",
        "\n",
        "model = DQN.load(\"dqn_pendulum\")\n",
        "\n",
        "obs = env.reset()\n",
        "while True:\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      obs = env.reset()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2478fa37bb82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gym/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gym/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gym/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "aE2N43SvtEKF",
        "outputId": "4cab4f5b-4880-4b67-e024-f438c2e4cda3"
      },
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import MlpPolicy\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# Parallel environments\n",
        "env = make_vec_env(CarRacing, n_envs=4)\n",
        "# env = gym.make(\"CarRacing-v0\")\n",
        "\n",
        "\n",
        "model = PPO(MlpPolicy, env, verbose=1)\n",
        "model.learn(total_timesteps=1)\n",
        "model.save(\"ppo_cartpole\")\n",
        "\n",
        "del model # remove to demonstrate saving and loading\n",
        "\n",
        "model = PPO.load(\"ppo_cartpole\")\n",
        "\n",
        "obs = env.reset()\n",
        "while True:\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.render()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "Track generation: 1111..1392 -> 281-tiles track\n",
            "Track generation: 1184..1484 -> 300-tiles track\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:417: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Track generation: 1130..1419 -> 289-tiles track\n",
            "retry to generate track (normal if there are not many of this messages)\n",
            "Track generation: 1156..1449 -> 293-tiles track\n",
            "Track generation: 1106..1387 -> 281-tiles track\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-702063f4cbe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMlpPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ppo_cartpole\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/vec_env/vec_transpose.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             )\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-72a69720a429>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state_pixels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mstep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-72a69720a429>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglViewport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVP_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVP_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_road\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-72a69720a429>\u001b[0m in \u001b[0;36mrender_road\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglVertex3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglVertex3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglVertex3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpoly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroad_poly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglColor4f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436,
          "referenced_widgets": [
            "d74dbc21f6a74b118728588f03cfbb93",
            "05ec633391c544dc914bb14e0f4b5879",
            "bd9972aaeb154e0d889a035fafad48f8",
            "667ed7b7be5c44d293ab87e798086993",
            "233b81de5b4c46d690ef011db8042583",
            "0e7c3b4c79294ee4ae87407ea0cfe09c",
            "53a0ed84b7dd41e7ae6bb83ae2347bde",
            "583a4db6d9074477b2930a37d49707e6"
          ]
        },
        "id": "c6li-P78pvKN",
        "outputId": "6d2fbf89-24d7-42f0-edaa-d867b9f002bd"
      },
      "source": [
        "wandb.init(resume=WANDB_ID)\n",
        "wandb.run.name = WNDB_NAME\n",
        "\n",
        "# load model\n",
        "model_artifact = wandb.use_artifact(MODEL_SAVE_NAME+':latest', type='model')\n",
        "artifact_dir = model_artifact.download()\n",
        "saved_model = torch.load(artifact_dir+\"/\"+MODEL_SAVE_NAME+\".pth\")\n",
        "saved_optimizer = torch.load(artifact_dir+\"/\"+OPTIMIZER_SAVE_NAME+\".pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:sexyid123_new2) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 6392<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d74dbc21f6a74b118728588f03cfbb93",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210219_235345-sexyid123_new2/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210219_235345-sexyid123_new2/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>_runtime</td><td>22077</td></tr><tr><td>_timestamp</td><td>1613778608</td></tr><tr><td>_step</td><td>73</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">Test_bruuuu_new_new2</strong>: <a href=\"https://wandb.ai/andreas_giannoutsos/gym_car_racer/runs/sexyid123_new2\" target=\"_blank\">https://wandb.ai/andreas_giannoutsos/gym_car_racer/runs/sexyid123_new2</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:sexyid123_new2). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.19<br/>\n",
              "                Resuming run <strong style=\"color:#cdcd00\">Test_bruuuu_new_new2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/andreas_giannoutsos/gym_car_racer\" target=\"_blank\">https://wandb.ai/andreas_giannoutsos/gym_car_racer</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/andreas_giannoutsos/gym_car_racer/runs/sexyid123_new2\" target=\"_blank\">https://wandb.ai/andreas_giannoutsos/gym_car_racer/runs/sexyid123_new2</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210220_000136-sexyid123_new2</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_PgrxrfbqVBp",
        "outputId": "5822574b-1d75-4fad-b195-c3c777d041eb"
      },
      "source": [
        "artifact_dir\n",
        "saved_model = torch.load(artifact_dir+\"/\"+MODEL_SAVE_NAME+\".pth\")\n",
        "saved_optimizer = torch.load(artifact_dir+\"/\"+OPTIMIZER_SAVE_NAME+\".pth\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./artifacts/DQN_test_model:v0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmPDDxMmoTvA"
      },
      "source": [
        "# env = gnwrapper.Animation(gym.make('CartPole-v1'))\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "# for _ in range(100):\n",
        "#     print(_)\n",
        "#     action = select_action(get_screen())\n",
        "#     next_obs, reward, done, info = env.step(action)\n",
        "#     env.render()\n",
        "\n",
        "#     obs = next_obs\n",
        "#     if done:\n",
        "#         obs = env.reset()\n",
        "#         print(_)\n",
        "total_steps = 0\n",
        "num_episodes = 10\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    # env = gnwrapper.Animation(gym.make('CartPole-v1'))\n",
        "    env = gym.make('CartPole-v1')\n",
        "\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "    print(i_episode)\n",
        "    for t in count():\n",
        "        total_steps+=1\n",
        "        # Select and perform an action\n",
        "        action = select_action(state)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # Observe new state\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        if not done:\n",
        "            next_state = current_screen - last_screen\n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            # plot_durations()\n",
        "            break\n",
        "\n",
        "env.close()\n",
        "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()\n",
        "print(\"total steps \", total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqLIwu80tabc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgvHtGJicRXW"
      },
      "source": [
        "Here is the diagram that illustrates the overall resulting data flow.\n",
        "\n",
        ".. figure:: /_static/img/reinforcement_learning_diagram.jpg\n",
        "\n",
        "Actions are chosen either randomly or based on a policy, getting the next\n",
        "step sample from the gym environment. We record the results in the\n",
        "replay memory and also run optimization step on every iteration.\n",
        "Optimization picks a random batch from the replay memory to do training of the\n",
        "new policy. \"Older\" target_net is also used in optimization to compute the\n",
        "expected Q values; it is updated occasionally to keep it current.\n",
        "\n",
        "\n"
      ]
    }
  ]
}